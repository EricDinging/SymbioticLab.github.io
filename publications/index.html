<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#fe6600">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
  <link rel="mask-icon" href="/assets/favicon/safari-pinned-tab.svg" color="#fe6600">
  <link rel="manifest" href="/assets/favicon/site.webmanifest">
  <meta name="msapplication-config" content="/assets/favicon/browserconfig.xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"symbioticlab.github.io","root":"/","scheme":"Mist","version":"8.0.0-rc.5","exturl":false,"sidebar":{"position":"right","display":"remove","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false};
  </script>

  <meta name="description" content="Work in progress                Filters:                                       All Venues                                      MLSys">
<meta property="og:type" content="website">
<meta property="og:title" content="Publications">
<meta property="og:url" content="https://symbioticlab.github.io/publications/index.html">
<meta property="og:site_name" content="SymbioticLab">
<meta property="og:description" content="Work in progress                Filters:                                       All Venues                                      MLSys">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-23T03:10:02.000Z">
<meta property="article:modified_time" content="2020-07-23T03:10:02.000Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://symbioticlab.github.io/publications/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>


  <title>Publications | SymbioticLab
</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SymbioticLab</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-projects">

    <a href="/projects" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-publications">

    <a href="/publications" rel="section"><i class="fa fa-book fa-fw"></i>Publications</a>

  </li>
        <li class="menu-item menu-item-people">

    <a href="/people" rel="section"><i class="fa fa-user fa-fw"></i>People</a>

  </li>
        <li class="menu-item menu-item-funding">

    <a href="/funding" rel="section"><i class="fa fa-info fa-fw"></i>Funding</a>

  </li>
  </ul>
</nav>




</div>
      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        
  
  

        <div class="content page posts-expand">
          

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Publications
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <div class="note warning"><p>Work in progress</p>
</div>
<div class="publist">
    <div class="timeline-search-panel">
        <h4>Filters:</h4>
        <label class="select-box">
            <select>
                <option value="all" selected>All Venues</option>
                <optgroup label="Conferences">
                    <option value="MLSys">MLSys</option>
                </optgroup>
            </select>
            <span class="select-arrow">
                <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                     version="1.1" viewBox="0 0 129 129"
                     enable-background="new 0 0 129 129"
                     width="512px" height="512px">
                    <g>
                        <path d="m121.3,34.6c-1.6-1.6-4.2-1.6-5.8,0l-51,51.1-51.1-51.1c-1.6-1.6-4.2-1.6-5.8,0-1.6,1.6-1.6,4.2 0,5.8l53.9,53.9c0.8,0.8 1.8,1.2 2.9,1.2 1,0 2.1-0.4 2.9-1.2l53.9-53.9c1.7-1.6 1.7-4.2 0.1-5.8z"/>
                    </g>
                </svg>
            </span>
        </label>
        <label class="check-box">
            <input type="checkbox" checked="checked" value="Conferences"> Conferences
        </label>
    </div> <!-- timeline-search-panel -->
    <div class="pub-list">
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2020"></span>
            </div>
            <ul>
                <li data-pub-venue="MLSys"
                    data-pub-cat="Conferences">
                    <div class="pub-block">
                        <div class="pub-title">
                            <a target="_blank" href="/publications/files/salus:mlsys20/salus-mlsys20.pdf">Fine-grained GPU sharing primitives for deep learning applications</a>
                            
                        </div>
                        <div class="pub-badges">
                            <span class="label default pub-badge">Artifacts Available</span>
                            <span class="label default pub-badge">Artifacts Evaluated Functional</span>
                            <span class="label default pub-badge">Artifacts Replicated</span>
                        </div>
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Peifeng-Yu ">Peifeng&nbspYu</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        <div class="pub-conference">
                            The 3rd Conference on Machine Learning and Systems
                            (<a target="_blank" title="19.2%"
                                href="https://mlsys.org/Conferences/2020">MLSys'20</a>)
                            <span class="pub-conference-acceptance">(Acceptance Rate: 19.2%)</span>
                        </div>
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote><p>Unlike traditional resources such as CPU or the network, modern GPUs do not natively support
fine-grained sharing primitives.
Consequently, implementing common policies such as time sharing and preemption are expensive. Worse,
when a deep learning (DL) application cannot completely use a GPU’s resources, the GPU cannot be efficiently shared
between multiple applications, leading to GPU underutilization.</p>
<p>We present Salus to enable two GPU sharing primitives: <strong>fast job
switching</strong> and <strong>memory sharing</strong>, to achieve fine-grained GPU sharing
among multiple DL applications. Salus is an efficient, consolidated
execution service that exposes the GPU to different DL applications, and it
enforces fine-grained sharing by performing iteration scheduling and
addressing associated memory management issues. We show that these primitives
can then be used to implement flexible sharing policies. Our integration of
Salus with TensorFlow and evaluation on popular DL jobs shows that Salus
can improve the average completion time of DL training jobs by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3.19</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">3.19\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">.</span><span class="mord">1</span><span class="mord">9</span><span class="mord">×</span></span></span></span>, GPU utilization for
hyper-parameter tuning by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2.38</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2.38\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">.</span><span class="mord">3</span><span class="mord">8</span><span class="mord">×</span></span></span></span>, and GPU utilization of DL inference applications by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>42</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">42\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mord">2</span><span class="mord">×</span></span></span></span> over not sharing
the GPU and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">6\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">6</span><span class="mord">×</span></span></span></span> over NVIDIA MPS with small overhead.</p>

                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a target="_blank" href="/publications/files/salus:mlsys20/salus-mlsys20.pdf">[paper]</a>
                            <a target="_blank" href="/publications/files/salus:mlsys20/salus-mlsys20-talk.pptx">[talk]</a>
                            <a target="_blank" href="/publications/files/salus:mlsys20/salus-mlsys20-poster.pdf">[poster]</a>
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{salus:mlsys20,
    author    = {Peifeng Yu and Mosharaf Chowdhury},
    booktitle = {MLSys},
    title     = {Fine-Grained {GPU} Sharing Primitives for Deep Learning Applications},
    year      = {2020},
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            AlloX: compute allocation in hybrid clusters
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Tan-N. Le ">Tan&nbspN.&nbspLe</span>, 
                            <span class="pub-author pub-author-Xiao-Sun ">Xiao&nbspSun</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Zhenhua-Liu ">Zhenhua&nbspLiu</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{allox:eurosys20,
    author    = {Tan N. Le and Xiao Sun and Mosharaf Chowdhury and Zhenhua Liu},
    booktitle = {ACM EuroSys},
    title     = {AlloX: compute allocation in hybrid clusters},
    year      = {2020},
    pages     = {31:1--31:16},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Sol: Fast distributed computation over slow networks
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Fan-Lai ">Fan&nbspLai</span>, 
                            <span class="pub-author pub-author-Jie-You ">Jie&nbspYou</span>, 
                            <span class="pub-author pub-author-Xiangfeng-Zhu ">Xiangfeng&nbspZhu</span>, 
                            <span class="pub-author pub-author-Harsha-V. Madhyastha ">Harsha&nbspV.&nbspMadhyastha</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{sol:nsdi20,
    author    = {Fan Lai and Jie You and Xiangfeng Zhu and Harsha V. Madhyastha and Mosharaf Chowdhury},
    booktitle = {USENIX NSDI},
    title     = {Sol: Fast Distributed Computation Over Slow Networks},
    year      = {2020},
    pages     = {273--288},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Near-optimal latency versus cost tradeoffs in geo-distributed storage
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Muhammed-Uluyol ">Muhammed&nbspUluyol</span>, 
                            <span class="pub-author pub-author-Anthony-Huang ">Anthony&nbspHuang</span>, 
                            <span class="pub-author pub-author-Ayush-Goel ">Ayush&nbspGoel</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Harsha-V. Madhyastha ">Harsha&nbspV.&nbspMadhyastha</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{pando:nsdi20,
    author    = {Muhammed Uluyol and Anthony Huang and Ayush Goel and Mosharaf Chowdhury and Harsha V. Madhyastha},
    booktitle = {USENIX NSDI},
    title     = {Near-Optimal Latency Versus Cost Tradeoffs in Geo-Distributed Storage},
    year      = {2020},
    pages     = {157--180},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            NetLock: Fast, centralized lock management using programmable switches
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Zhuolong-Yu ">Zhuolong&nbspYu</span>, 
                            <span class="pub-author pub-author-Yiwen-Zhang ">Yiwen&nbspZhang</span>, 
                            <span class="pub-author pub-author-Vladimir-Braverman ">Vladimir&nbspBraverman</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Xin-Jin ">Xin&nbspJin</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{netlock:sigcomm20,
    author    = {Zhuolong Yu and Yiwen Zhang and Vladimir Braverman and Mosharaf Chowdhury and Xin Jin},
    booktitle = {ACM SIGCOMM},
    title     = {NetLock: Fast, Centralized Lock Management Using Programmable Switches},
    year      = {2020},
    pages     = {126--138},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Effectively prefetching remote memory with leap
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Hasan-Al Maruf ">Hasan&nbspAl&nbspMaruf</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{leap:atc20,
    author    = {Hasan Al Maruf and Mosharaf Chowdhury},
    booktitle = {USENIX ATC},
    title     = {Effectively Prefetching Remote Memory with Leap},
    year      = {2020},
    pages     = {843--857},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2019"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Tiresias: A GPU cluster manager for distributed deep learning
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Juncheng-Gu ">Juncheng&nbspGu</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Kang-G. Shin ">Kang&nbspG.&nbspShin</span>, 
                            <span class="pub-author pub-author-Yibo-Zhu ">Yibo&nbspZhu</span>, 
                            <span class="pub-author pub-author-Myeongjae-Jeon ">Myeongjae&nbspJeon</span>, 
                            <span class="pub-author pub-author-Junjie-Qian ">Junjie&nbspQian</span>, 
                            <span class="pub-author pub-author-Hongqiang-Harry Liu ">Hongqiang&nbspHarry&nbspLiu</span>, and 
                            <span class="pub-author pub-author-Chuanxiong-Guo ">Chuanxiong&nbspGuo</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{tiresias:nsdi19,
    author    = {Juncheng Gu and Mosharaf Chowdhury and Kang G. Shin and Yibo Zhu and Myeongjae Jeon and Junjie Qian and Hongqiang Harry Liu and Chuanxiong Guo},
    booktitle = {USENIX NSDI},
    title     = {Tiresias: {A} {GPU} Cluster Manager for Distributed Deep Learning},
    year      = {2019},
    pages     = {485--500},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Near optimal coflow scheduling in networks
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Samir-Khuller ">Samir&nbspKhuller</span>, 
                            <span class="pub-author pub-author-Manish-Purohit ">Manish&nbspPurohit</span>, 
                            <span class="pub-author pub-author-Sheng-Yang ">Sheng&nbspYang</span>, and 
                            <span class="pub-author pub-author-Jie-You ">Jie&nbspYou</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{nocs:spaa19,
    author    = {Mosharaf Chowdhury and Samir Khuller and Manish Purohit and Sheng Yang and Jie You},
    booktitle = {ACM SPAA},
    title     = {Near Optimal Coflow Scheduling in Networks},
    year      = {2019},
    pages     = {123--134},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Salus: Fine-grained GPU sharing primitives for deep learning applications
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Peifeng-Yu ">Peifeng&nbspYu</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{salus:arxiv19,
    author        = {Peifeng Yu and Mosharaf Chowdhury},
    journal       = {CoRR},
    title         = {Salus: Fine-Grained {GPU} Sharing Primitives for Deep Learning Applications},
    year          = {2019},
    volume        = {abs/1902.04610},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1902-04610.bib},
    eprint        = {1902.04610},
    url           = {http://arxiv.org/abs/1902.04610},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Terra: Scalable cross-layer GDA optimizations
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Jie-You ">Jie&nbspYou</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{terra:arxiv19,
    author        = {Jie You and Mosharaf Chowdhury},
    journal       = {CoRR},
    title         = {Terra: Scalable Cross-Layer {GDA} Optimizations},
    year          = {2019},
    volume        = {abs/1904.08480},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1904-08480.bib},
    eprint        = {1904.08480},
    url           = {http://arxiv.org/abs/1904.08480},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            RDMA performance isolation with justitia
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Yiwen-Zhang ">Yiwen&nbspZhang</span>, 
                            <span class="pub-author pub-author-Yue-Tan ">Yue&nbspTan</span>, 
                            <span class="pub-author pub-author-Brent-Stephens ">Brent&nbspStephens</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{justitia:arxiv19,
    author        = {Yiwen Zhang and Yue Tan and Brent Stephens and Mosharaf Chowdhury},
    journal       = {CoRR},
    title         = {{RDMA} Performance Isolation With Justitia},
    year          = {2019},
    volume        = {abs/1905.04437},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1905-04437.bib},
    eprint        = {1905.04437},
    url           = {http://arxiv.org/abs/1905.04437},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Near optimal coflow scheduling in networks
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Samir-Khuller ">Samir&nbspKhuller</span>, 
                            <span class="pub-author pub-author-Manish-Purohit ">Manish&nbspPurohit</span>, 
                            <span class="pub-author pub-author-Sheng-Yang ">Sheng&nbspYang</span>, and 
                            <span class="pub-author pub-author-Jie-You ">Jie&nbspYou</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{nocs:arxiv19,
    author        = {Mosharaf Chowdhury and Samir Khuller and Manish Purohit and Sheng Yang and Jie You},
    journal       = {CoRR},
    title         = {Near Optimal Coflow Scheduling in Networks},
    year          = {2019},
    volume        = {abs/1906.06851},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1906-06851.bib},
    eprint        = {1906.06851},
    url           = {http://arxiv.org/abs/1906.06851},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Mitigating the performance-efficiency tradeoff in resilient memory disaggregation
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Youngmoon-Lee ">Youngmoon&nbspLee</span>, 
                            <span class="pub-author pub-author-Hassan-Al Maruf ">Hassan&nbspAl&nbspMaruf</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Kang-G. Shin ">Kang&nbspG.&nbspShin</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{hydra:arxiv19,
    author        = {Youngmoon Lee and Hassan Al Maruf and Mosharaf Chowdhury and Kang G. Shin},
    journal       = {CoRR},
    title         = {Mitigating the Performance-Efficiency Tradeoff in Resilient Memory Disaggregation},
    year          = {2019},
    volume        = {abs/1910.09727},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1910-09727.bib},
    eprint        = {1910.09727},
    url           = {http://arxiv.org/abs/1910.09727},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Effectively prefetching remote memory with leap
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Hasan-Al Maruf ">Hasan&nbspAl&nbspMaruf</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{leap:arxiv19,
    author        = {Hasan Al Maruf and Mosharaf Chowdhury},
    journal       = {CoRR},
    title         = {Effectively Prefetching Remote Memory with Leap},
    year          = {2019},
    volume        = {abs/1911.09829},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1911-09829.bib},
    eprint        = {1911.09829},
    url           = {http://arxiv.org/abs/1911.09829},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            BoPF: Mitigating the burstiness-fairness tradeoff in multi-resource clusters
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Tan-N. Le ">Tan&nbspN.&nbspLe</span>, 
                            <span class="pub-author pub-author-Xiao-Sun ">Xiao&nbspSun</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Zhenhua-Liu ">Zhenhua&nbspLiu</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{bopf:arxiv19,
    author        = {Tan N. Le and Xiao Sun and Mosharaf Chowdhury and Zhenhua Liu},
    journal       = {CoRR},
    title         = {BoPF: Mitigating the Burstiness-Fairness Tradeoff in Multi-Resource Clusters},
    year          = {2019},
    volume        = {abs/1912.03523},
    archiveprefix = {arXiv},
    bibsource     = {dblp computer science bibliography, https://dblp.org},
    biburl        = {https://dblp.org/rec/journals/corr/abs-1912-03523.bib},
    eprint        = {1912.03523},
    url           = {http://arxiv.org/abs/1912.03523},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2018"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Pas de deux: Shape the Circuits, and Shape the Apps too!
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Hong-Zhang ">Hong&nbspZhang</span>, 
                            <span class="pub-author pub-author-Kai-Chen ">Kai&nbspChen</span>, and 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{pdd:apnet18,
    author    = {Hong Zhang and Kai Chen and Mosharaf Chowdhury},
    booktitle = {ACM APNet},
    title     = {Pas de deux: Shape the Circuits, and Shape the Apps too!},
    year      = {2018},
    pages     = {29--35},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Bridging the GAP: Towards approximate graph analytics
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Anand-Padmanabha Iyer ">Anand&nbspPadmanabha&nbspIyer</span>, 
                            <span class="pub-author pub-author-Aurojit-Panda ">Aurojit&nbspPanda</span>, 
                            <span class="pub-author pub-author-Shivaram-Venkataraman ">Shivaram&nbspVenkataraman</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Aditya-Akella ">Aditya&nbspAkella</span>, 
                            <span class="pub-author pub-author-Scott-Shenker ">Scott&nbspShenker</span>, and 
                            <span class="pub-author pub-author-Ion-Stoica ">Ion&nbspStoica</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{aga:grades-nda18,
    author    = {Anand Padmanabha Iyer and Aurojit Panda and Shivaram Venkataraman and Mosharaf Chowdhury and Aditya Akella and Scott Shenker and Ion Stoica},
    booktitle = {ACM SIGMOD GRADES-NDA},
    title     = {Bridging the {GAP}: Towards Approximate Graph analytics},
    year      = {2018},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Monarch: Gaining command on geo-distributed graph analytics
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Anand-Padmanabha Iyer ">Anand&nbspPadmanabha&nbspIyer</span>, 
                            <span class="pub-author pub-author-Aurojit-Panda ">Aurojit&nbspPanda</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Aditya-Akella ">Aditya&nbspAkella</span>, 
                            <span class="pub-author pub-author-Scott-Shenker ">Scott&nbspShenker</span>, and 
                            <span class="pub-author pub-author-Ion-Stoica ">Ion&nbspStoica</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{monarch:hotcloud18,
    author    = {Anand Padmanabha Iyer and Aurojit Panda and Mosharaf Chowdhury and Aditya Akella and Scott Shenker and Ion Stoica},
    booktitle = {USENIX HotCloud},
    title     = {Monarch: Gaining Command on Geo-Distributed Graph Analytics},
    year      = {2018},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            To relay or not to relay for inter-cloud transfers?
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Fan-Lai ">Fan&nbspLai</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Harsha-V. Madhyastha ">Harsha&nbspV.&nbspMadhyastha</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{relay:hotcloud18,
    author    = {Fan Lai and Mosharaf Chowdhury and Harsha V. Madhyastha},
    booktitle = {USENIX HotCloud},
    title     = {To Relay or Not to Relay for Inter-Cloud Transfers?},
    year      = {2018},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Mitigating the latency-accuracy trade-off in mobile data analytics systems
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Anand-Padmanabha Iyer ">Anand&nbspPadmanabha&nbspIyer</span>, 
                            <span class="pub-author pub-author-Li-Erran Li ">Li&nbspErran&nbspLi</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Ion-Stoica ">Ion&nbspStoica</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{cellscope:mobicom18,
    author    = {Anand Padmanabha Iyer and Li Erran Li and Mosharaf Chowdhury and Ion Stoica},
    booktitle = {ACM MobiCom},
    title     = {Mitigating the Latency-Accuracy Trade-off in Mobile Data Analytics Systems},
    year      = {2018},
    pages     = {513--528},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Dynamic query re-planning using QOOP
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Kshiteej-Mahajan ">Kshiteej&nbspMahajan</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Aditya-Akella ">Aditya&nbspAkella</span>, and 
                            <span class="pub-author pub-author-Shuchi-Chawla ">Shuchi&nbspChawla</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{qoop:osdi18,
    author    = {Kshiteej Mahajan and Mosharaf Chowdhury and Aditya Akella and Shuchi Chawla},
    booktitle = {USENIX OSDI},
    title     = {Dynamic Query Re-Planning using {QOOP}},
    year      = {2018},
    pages     = {253--267},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Distributed lock management with RDMA: Decentralization without starvation
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Dong-Young Yoon ">Dong&nbspYoung&nbspYoon</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Barzan-Mozafari ">Barzan&nbspMozafari</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{dslr:sigmod18,
    author    = {Dong Young Yoon and Mosharaf Chowdhury and Barzan Mozafari},
    booktitle = {ACM SIGMOD},
    title     = {Distributed Lock Management with {RDMA}: Decentralization without Starvation},
    year      = {2018},
    pages     = {1571--1586},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Fair allocation of heterogeneous and interchangeable resources
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Xiao-Sun ">Xiao&nbspSun</span>, 
                            <span class="pub-author pub-author-Tan-N. Le ">Tan&nbspN.&nbspLe</span>, 
                            <span class="pub-author pub-author-Mosharaf-Chowdhury ">Mosharaf&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Zhenhua-Liu ">Zhenhua&nbspLiu</span>
                        </div>
                        
                        
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{allox:mama18,
    author    = {Xiao Sun and Tan N. Le and Mosharaf Chowdhury and Zhenhua Liu},
    booktitle = {ACM SIGMETRICS MAMA},
    title     = {Fair Allocation of Heterogeneous and Interchangeable Resources},
    year      = {2018},
}
">[bibtex]</a>
                            
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2017"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Decentralized memory disaggregation over low-latency networks
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-J.-Gu ">J.&nbspGu</span>, 
                            <span class="pub-author pub-author-Y.-Lee ">Y.&nbspLee</span>, 
                            <span class="pub-author pub-author-Y.-Zhang ">Y.&nbspZhang</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-K.-G. Shin ">K.&nbspG.&nbspShin</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Memory disaggregation can expose remote memory across a cluster to local applications. However, existing proposals call for new architectures and/or new programming models, making them infeasible. We have developed a practical memory disaggregation solution, Infiniswap, which is a remote memory paging system for clusters with lowlatency, kernel-bypass networks such as RDMA. Infiniswap opportunistically harvests and transparently exposes unused memory across the cluster to unmodified applications by dividing the swap space of each machine into many chunks and distributing them to unused memory of many remote machines. For scalability, it leverages the power of many choices to perform decentralized memory chunk placements and evictions. Applications using Infiniswap receive large performance boosts when their working sets are larger than their physical memory allocations.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{infiniswap:login17,
    Title                    = {Decentralized Memory Disaggregation Over Low-Latency Networks},
    Author                   = {J. Gu and Y. Lee and Y. Zhang and M. Chowdhury and K. G. Shin},
    Journal                  = {USENIX ;login:},
    Year                     = {2017},
    Month                    = {December},
    Number                   = {4},
    Pages                    = {42--48},
    Volume                   = {42},
    Abstract                 = {Memory disaggregation can expose remote memory across a cluster to local applications. However, existing proposals call for new architectures and/or new programming models, making them infeasible. We have developed a practical memory disaggregation solution, Infiniswap, which is a remote memory paging system for clusters with lowlatency, kernel-bypass networks such as RDMA. Infiniswap opportunistically harvests and transparently exposes unused memory across the cluster to unmodified applications by dividing the swap space of each machine into many chunks and distributing them to unused memory of many remote machines. For scalability, it leverages the power of many choices to perform decentralized memory chunk placements and evictions. Applications using Infiniswap receive large performance boosts when their working sets are larger than their physical memory allocations.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Resilient datacenter load balancing in the wild
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-H.-Zhang ">H.&nbspZhang</span>, 
                            <span class="pub-author pub-author-J.-Zhang ">J.&nbspZhang</span>, 
                            <span class="pub-author pub-author-W.-Bai K. Chen ">W.&nbspBai&nbspK.&nbspChen</span>, and 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallouts. Despite significant efforts, prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions such as CONGA and CLOVE can sense congestion, but they can only reroute when flowlets emerge; thus, they cannot always react timely to uncertainties. To make things worse, these solutions fail to detect/handle failures such as blackholes and random packet drops, which greatly degrades their performance. In this paper, we introduce Hermes, a datacenter load balancer that is resilient to the aforementioned uncertainties. At its heart, Hermes leverages comprehensive sensing to detect path conditions including failures unattended before, and it reacts using timely yet cautious rerouting. Hermes is a practical edge-based solution with no switch modification. We have implemented Hermes with commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and well handles uncertainties: under asymmetries, Hermes achieves up to 10% and 20% better flow completion time (FCT) than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{hermes:sigcomm17,
    Title                    = {Resilient Datacenter Load Balancing in the Wild},
    Author                   = {H. Zhang and J. Zhang and W. Bai K. Chen and M. Chowdhury},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2017},
    Month                    = {August},
    Abstract                 = {Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallouts. Despite significant efforts, prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions such as CONGA and CLOVE can sense congestion, but they can only reroute when flowlets emerge; thus, they cannot always react timely to uncertainties. To make things worse, these solutions fail to detect/handle failures such as blackholes and random packet drops, which greatly degrades their performance.

In this paper, we introduce Hermes, a datacenter load balancer that is resilient to the aforementioned uncertainties. At its heart, Hermes leverages comprehensive sensing to detect path conditions including failures unattended before, and it reacts using timely yet cautious rerouting. Hermes is a practical edge-based solution with no switch modification. We have implemented Hermes with commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and well handles uncertainties: under asymmetries, Hermes achieves up to 10% and 20% better flow completion time (FCT) than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Performance isolation anomalies in RDMA
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-Y.-Zhang ">Y.&nbspZhang</span>, 
                            <span class="pub-author pub-author-J.-Gu ">J.&nbspGu</span>, 
                            <span class="pub-author pub-author-Y.-Lee ">Y.&nbspLee</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-K.-G. Shin ">K.&nbspG.&nbspShin</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>To meet the increasing throughput and latency demands of modern applications, many operators are rapidly deploying RDMA in their datacenters. At the same time, developers are re-designing their software to take advantage of RDMA's benefits for individual applications. However, when it comes to RDMA's performance, many simple questions remain open. In this paper, we consider the performance isolation characteristics of RDMA. Specifically, we conduct three sets of experiments – three combinations of one throughput-sensitive flow and one latency-sensitive flow – in a controlled environment, observe large discrepancies in RDMA performance with and without the presence of a competing flow, and describe our progress in identifying plausible root-causes.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{fairdma:kbnets2017,
    Title                    = {Performance Isolation Anomalies in {RDMA}},
    Author                   = {Y. Zhang and J. Gu and Y. Lee and M. Chowdhury and K. G. Shin},
    Booktitle                = {ACM SIGCOMMKBNets},
    Year                     = {2017},
    Month                    = {August},
    Abstract                 = {To meet the increasing throughput and latency demands of modern applications, many operators are rapidly deploying RDMA in their datacenters. At the same time, developers are re-designing their software to take advantage of RDMA's benefits for individual applications. However, when it comes to RDMA's performance, many simple questions remain open.

In this paper, we consider the performance isolation characteristics of RDMA. Specifically, we conduct three sets of experiments -- three combinations of one throughput-sensitive flow and one latency-sensitive flow -- in a controlled environment, observe large discrepancies in RDMA performance with and without the presence of a competing flow, and describe our progress in identifying plausible root-causes.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            No! Not another deep learning framework
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-L.-Nguyen ">L.&nbspNguyen</span>, 
                            <span class="pub-author pub-author-P.-Yu ">P.&nbspYu</span>, and 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>In recent years, deep learning has pervaded many areas of computing due to the confluence of an explosive growth of large-scale computing capabilities, availability of datasets, and advances in learning techniques. While this rapid growth has resulted in diverse deep learning frameworks, it has also led to inefficiencies for both the users and developers of these frameworks. Specifically, adopting useful techniques across frameworks – both to perform learning tasks and to optimize performance – involves significant repetitions and reinventions. In this paper, we observe that despite their diverse origins, many of these frameworks share architectural similarities. We argue that by introducing a common representation of learning tasks and a hardware abstraction model to capture compute heterogeneity, we might be able to relieve machine learning researchers from dealing with low-level systems issues and systems researchers from being tied to any specific framework. We expect this decoupling to accelerate progress in both domains.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{deepstack:hotos17,
    Title                    = {No! Not Another Deep Learning Framework},
    Author                   = {L. Nguyen and P. Yu and M. Chowdhury},
    Booktitle                = {ACM HotOS},
    Year                     = {2017},
    Month                    = {May},
    Abstract                 = {In recent years, deep learning has pervaded many areas of computing due to the confluence of an explosive growth of large-scale computing capabilities, availability of datasets, and advances in learning techniques. While this rapid growth has resulted in diverse deep learning frameworks, it has also led to inefficiencies for both the users and developers of these frameworks. Specifically, adopting useful techniques across frameworks -- both to perform learning tasks and to optimize performance -- involves significant repetitions and reinventions.

In this paper, we observe that despite their diverse origins, many of these frameworks share architectural similarities. We argue that by introducing a common representation of learning tasks and a hardware abstraction model to capture compute heterogeneity, we might be able to relieve machine learning researchers from dealing with low-level systems issues and systems researchers from being tied to any specific framework. We expect this decoupling to accelerate progress in both domains.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Efficient memory disaggregation with Infiniswap
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-J.-Gu ">J.&nbspGu</span>, 
                            <span class="pub-author pub-author-Y.-Lee ">Y.&nbspLee</span>, 
                            <span class="pub-author pub-author-Y.-Zhang ">Y.&nbspZhang</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-K.-G. Shin ">K.&nbspG.&nbspShin</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Memory-intensive applications suffer large performance loss when their working sets do not fully fit in memory. Yet, they cannot leverage otherwise unused remote memory when paging out to disks even in the presence of large imbalance in memory utilizations across a cluster. Existing proposals for memory disaggregation call for new architectures, new hardware designs, and/or new programming models, making them infeasible. This paper describes the design and implementation of Infiniswap, a remote memory paging system designed specifically for an RDMA network. Infiniswap opportunistically harvests and transparently exposes unused memory to unmodified applications by dividing the swap space of each machine into many slabs and distributing them across many machines' remote memory. Because one-sided RDMA operations bypass remote CPUs, Infiniswap leverages the power of many choices to perform decentralized slab placements and evictions. We have implemented and deployed Infiniswap on an RDMA cluster without any modifications to user applications or the OS and evaluated its effectiveness using multiple workloads running on unmodified VoltDB, Memcached, PowerGraph, GraphX, and Apache Spark. Using Infiniswap, throughputs of these applications improve between 4X (0.94X) to 15.4X (7.8X) over disk (Mellanox nbdX), and median and tail latencies between 5.4X (2X) and 61X (2.3X). Infiniswap achieves these with negligible remote CPU usage, whereas nbdX becomes CPU-bound. Infiniswap increases the overall memory utilization of a cluster and works well at scale.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{infiniswap:nsdi17,
    Title                    = {Efficient Memory Disaggregation with {Infiniswap}},
    Author                   = {J. Gu and Y. Lee and Y. Zhang and M. Chowdhury and K. G. Shin},
    Booktitle                = {USENIX NSDI},
    Year                     = {2017},
    Month                    = {March},
    Abstract                 = {Memory-intensive applications suffer large performance loss when their working sets do not fully fit in memory. Yet, they cannot leverage otherwise unused remote memory when paging out to disks even in the presence of large imbalance in memory utilizations across a cluster. Existing proposals for memory disaggregation call for new architectures, new hardware designs, and/or new programming models, making them infeasible.

This paper describes the design and implementation of Infiniswap, a remote memory paging system designed specifically for an RDMA network. Infiniswap opportunistically harvests and transparently exposes unused memory to unmodified applications by dividing the swap space of each machine into many slabs and distributing them across many machines' remote memory. Because one-sided RDMA operations bypass remote CPUs, Infiniswap leverages the power of many choices to perform decentralized slab placements and evictions.

We have implemented and deployed Infiniswap on an RDMA cluster without any modifications to user applications or the OS and evaluated its effectiveness using multiple workloads running on unmodified VoltDB, Memcached, PowerGraph, GraphX, and Apache Spark. Using Infiniswap, throughputs of these applications improve between 4X (0.94X) to 15.4X (7.8X) over disk (Mellanox nbdX), and median and tail latencies between 5.4X (2X) and 61X (2.3X). Infiniswap achieves these with negligible remote CPU usage, whereas nbdX becomes CPU-bound. Infiniswap increases the overall memory utilization of a cluster and works well at scale.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2016"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Altruistic scheduling in multi-resource clusters
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-R.-Grandl ">R.&nbspGrandl</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-A.-Akella ">A.&nbspAkella</span>, and 
                            <span class="pub-author pub-author-G.-Ananthanarayanan ">G.&nbspAnanthanarayanan</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Given the well-known tradeoffs between fairness, performance, and efficiency, modern cluster schedulers often prefer instantaneous fairness as their primary objective to ensure performance isolation between users and groups. However, instantaneous, short-term convergence to fairness often does not result in noticeable long-term benefits. Instead, we propose an altruistic, long-term approach, Carbyne, where jobs yield fractions of their allocated resources without impacting their own completion times. We show that leftover resources collected via altruisms of many jobs can then be rescheduled to further secondary goals such as application-level performance and cluster efficiency without impacting performance isolation. Deployments and large-scale simulations show that Carbyne closely approximates the state-of-the-art solutions (e.g., DRF [27]) in terms of performance isolation, while providing 1.26X better efficiency and 1.59X lower average job completion time.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{carbyne:osdi16,
    Title                    = {Altruistic Scheduling in Multi-Resource Clusters},
    Author                   = {R. Grandl and M. Chowdhury and A. Akella and G. Ananthanarayanan},
    Booktitle                = {USENIX OSDI},
    Year                     = {2016},
    Month                    = {October},
    Abstract                 = {Given the well-known tradeoffs between fairness, performance, and efficiency, modern cluster schedulers often prefer instantaneous fairness as their primary objective to ensure performance isolation between users and groups. However, instantaneous, short-term convergence to fairness often does not result in noticeable long-term benefits. Instead, we propose an altruistic, long-term approach, Carbyne, where jobs yield fractions of their allocated resources without impacting their own completion times. We show that leftover resources collected via altruisms of many jobs can then be rescheduled to further secondary goals such as application-level performance and cluster efficiency without impacting performance isolation. Deployments and large-scale simulations show that Carbyne closely approximates the state-of-the-art solutions (e.g., DRF [27]) in terms of performance isolation, while providing 1.26X better efficiency and 1.59X lower average job completion time.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            EC-Cache: Load-balanced, low-latency cluster caching with online erasure coding
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-K.-V. Rashmi ">K.&nbspV.&nbspRashmi</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-J.-Kosaian ">J.&nbspKosaian</span>, 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>, and 
                            <span class="pub-author pub-author-K.-Ramchandran ">K.&nbspRamchandran</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Data-intensive clusters and object stores are increasingly relying on in-memory object caching to meet the I/O performance demands. These systems routinely face the challenges of popularity skew, background load imbalance, and server failures, which result in severe load imbalance across storage servers and degraded I/O performance. Selective replication is a commonly used technique to tackle these challenges, where the number of cached replicas of an object is proportional to its popularity. In this paper, we explore an alternative approach using erasure coding. EC-Cache is a load-balanced, low latency cluster cache that uses online erasure coding to overcome the limitations of selective replication. EC-Cache employs erasure coding by: (i) splitting and erasure coding individual objects during writes, and (ii) late binding, wherein obtaining any k out of (k + r) splits of an object are sufficient, during reads. As compared to selective replication, EC-Cache improves load balancing by more than 3X and reduces the median and tail read latencies by more than 2X, while using the same amount of memory. EC-Cache does so using 10% additional bandwidth and a small increase in the amount of stored metadata. The benefits offered by EC-Cache are further amplified in the presence of background network load imbalance and server failures.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{eccache:osdi16,
    Title                    = {{EC-Cache}: Load-Balanced, Low-Latency Cluster Caching with Online Erasure Coding},
    Author                   = {K. V. Rashmi and M. Chowdhury and J. Kosaian and I. Stoica and K. Ramchandran},
    Booktitle                = {USENIX OSDI},
    Year                     = {2016},
    Month                    = {October},
    Abstract                 = {Data-intensive clusters and object stores are increasingly relying on in-memory object caching to meet the I/O performance demands. These systems routinely face the challenges of popularity skew, background load imbalance, and server failures, which result in severe load imbalance across storage servers and degraded I/O performance. Selective replication is a commonly used technique to tackle these challenges, where the number of cached replicas of an object is proportional to its popularity. In this paper, we explore an alternative approach using erasure coding.

EC-Cache is a load-balanced, low latency cluster cache that uses online erasure coding to overcome the limitations of selective replication. EC-Cache employs erasure coding by: (i) splitting and erasure coding individual objects during writes, and (ii) late binding, wherein obtaining any k out of (k + r) splits of an object are sufficient, during reads. As compared to selective replication, EC-Cache improves load balancing by more than 3X and reduces the median and tail read latencies by more than 2X, while using the same amount of memory. EC-Cache does so using 10% additional bandwidth and a small increase in the amount of stored metadata. The benefits offered by EC-Cache are further amplified in the presence of background network load imbalance and server failures.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            CODA: Toward automatically identifying and scheduling COflows in the DArk
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-H.-Zhang ">H.&nbspZhang</span>, 
                            <span class="pub-author pub-author-L.-Chen ">L.&nbspChen</span>, 
                            <span class="pub-author pub-author-B.-Yi ">B.&nbspYi</span>, 
                            <span class="pub-author pub-author-K.-Chen ">K.&nbspChen</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-Y.-Geng ">Y.&nbspGeng</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Leveraging application-level requirements using coflows has recently been shown to improve application-level communication performance in data-parallel clusters. However, existing coflow-based solutions rely on modifying applications to extract coflows, making them inapplicable to many practical scenarios. In this paper, we present CODA, a first attempt at automatically identifying and scheduling coflows without any application modifications. We employ an incremental clustering algorithm to perform fast, application-transparent coflow identification and complement it by proposing an error-tolerant coflow scheduler to mitigate occasional identification errors. Testbed experiments and large-scale simulations with production workloads show that CODA can identify coflows with over 90% accuracy, and its scheduler is robust to inaccuracies, enabling communication stages to complete 2.4X (5.1X) faster on average (95th percentile) compared to per-flow mechanisms. Overall, CODA�s performance is comparable to that of solutions requiring application modifications.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{coda:sigcomm16,
    Title                    = {{CODA}: Toward Automatically Identifying and Scheduling {CO}flows in the {DA}rk},
    Author                   = {H. Zhang and L. Chen and B. Yi and K. Chen and M. Chowdhury and Y. Geng},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2016},
    Month                    = {August},
    Abstract                 = {Leveraging application-level requirements using coflows has recently been shown to improve application-level communication performance in data-parallel clusters. However, existing coflow-based solutions rely on modifying applications to extract coflows, making them inapplicable to many practical scenarios.

In this paper, we present CODA, a first attempt at automatically identifying and scheduling coflows without any application modifications. We employ an incremental clustering algorithm to perform fast, application-transparent coflow identification and complement it by proposing an error-tolerant coflow scheduler to mitigate occasional identification errors. Testbed experiments and large-scale simulations with production workloads show that CODA can identify coflows with over 90% accuracy, and its scheduler is robust to inaccuracies, enabling communication stages to complete 2.4X (5.1X) faster on average (95th percentile) compared to per-flow mechanisms. Overall, CODA�s performance is comparable to that of solutions requiring application modifications.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Fast and accurate performance analysis of LTE radio access networks
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-A.-P. Iyer ">A.&nbspP.&nbspIyer</span>, 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-L.-E. Li ">L.&nbspE.&nbspLi</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5x to 4.4x while reducing the model update overhead by up to 4.8x. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@techreport{cellscope:tr16,
    Title                    = {Fast and Accurate Performance Analysis of {LTE} Radio Access Networks},
    Author                   = {A. P. Iyer and I. Stoica and M. Chowdhury and L. E. Li},
    Institution              = {CoRR},
    Year                     = {2016},
    Month                    = {May},
    Number                   = {abs/1605.04652},
    Abstract                 = {An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy.

In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5x to 4.4x while reducing the model update overhead by up to 4.8x. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            HUG: Multi-resource fairness for correlated and elastic demands
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Z.-Liu ">Z.&nbspLiu</span>, 
                            <span class="pub-author pub-author-A.-Ghodsi ">A.&nbspGhodsi</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>In this paper, we study how to optimally provide isolation guarantees in multi-resource environments, such as public clouds, where a tenant's demands on different resources (links) are correlated. Unlike prior work such as Dominant Resource Fairness (DRF) that assumes static and fixed demands, we consider elastic demands. Our approach generalizes canonical max-min fairness to the multi-resource setting with correlated demands, and extends DRF to elastic demands. We consider two natural optimization objectives: isolation guarantee from a tenant's viewpoint and system utilization (work conservation) from an operator's perspective. We prove that in non-cooperative environments like public cloud networks, there is a strong tradeoff between optimal isolation guarantee and work conservation when demands are elastic. Even worse, work conservation can even decrease network utilization instead of improving it when demands are inelastic. We identify the root cause behind the tradeoff and present a provably optimal allocation algorithm, High Utilization with Guarantees (HUG), to achieve maximum attainable network utilization without sacrificing the optimal isolation guarantee, strategy-proofness, and other useful properties of DRF. In cooperative environments like private datacenter networks, HUG achieves both the optimal isolation guarantee and work conservation. Analyses, simulations, and experiments show that HUG provides better isolation guarantees, higher system utilization, and better tenant-level performance than its counterparts.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{hug:nsdi16,
    Title                    = {{HUG}: Multi-Resource Fairness for Correlated and Elastic Demands},
    Author                   = {M. Chowdhury and Z. Liu and A. Ghodsi and I. Stoica},
    Booktitle                = {USENIX NSDI},
    Year                     = {2016},
    Month                    = {March},
    Abstract                 = {In this paper, we study how to optimally provide isolation guarantees in multi-resource environments, such as public clouds, where a tenant's demands on different resources (links) are correlated. Unlike prior work such as Dominant Resource Fairness (DRF) that assumes static and fixed demands, we consider elastic demands. Our approach generalizes canonical max-min fairness to the multi-resource setting with correlated demands, and extends DRF to elastic demands. We consider two natural optimization objectives: isolation guarantee from a tenant's viewpoint and system utilization (work conservation) from an operator's perspective. We prove that in non-cooperative environments like public cloud networks, there is a strong tradeoff between optimal isolation guarantee and work conservation when demands are elastic. Even worse, work conservation can even decrease network utilization instead of improving it when demands are inelastic. We identify the root cause behind the tradeoff and present a provably optimal allocation algorithm, High Utilization with Guarantees (HUG), to achieve maximum attainable network utilization without sacrificing the optimal isolation guarantee, strategy-proofness, and other useful properties of DRF. In cooperative environments like private datacenter networks, HUG achieves both the optimal isolation guarantee and work conservation. Analyses, simulations, and experiments show that HUG provides better isolation guarantees, higher system utilization, and better tenant-level performance than its counterparts.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2015"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Coflow: A networking abstraction for distributed data-parallel applications
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Over the past decade, the confluence of an unprecedented growth in data volumes and the rapid rise of cloud computing has fundamentally transformed systems software and corresponding infrastructure. To deal with massive datasets, more and more applications today are scaling out to large datacenters. These distributed data-parallel applications run on tens to thousands of machines in parallel to exploit I/O parallelism, and they enable a wide variety of use cases, including interactive analysis, SQL queries, machine learning, and graph processing. Communication between the distributed computation tasks of these applications often result in massive data transfers over the network. Consequently, concentrated efforts in both industry and academia have gone into building high-capacity, low-latency datacenter networks at scale. At the same time, researchers and practitioners have proposed a wide variety of solutions to minimize flow completion times or to ensure per-flow fairness based on the point-to-point flow abstraction that forms the basis of the TCP/IP stack. We observe that despite rapid innovations in both applications and infrastructure, application- and network-level goals are moving further apart. Data-parallel applications care about all their flows, but today�s networks treat each point-to-point flow independently. This fundamental mismatch has resulted in complex point solutions for application developers, a myriad of configuration options for end users, and an overall loss of performance. The key contribution of this dissertation is bridging this gap between application-level performance and network-level optimizations through the coflow abstraction. Each multipoint-to-multipoint coflow represents a collection of flows with a common application-level performance objective, enabling application-aware decision making in the network. We describe complete solutions including architectures, algorithms, and implementations that apply coflows to multiple scenarios using central coordination, and we demonstrate through large-scale cloud deployments and trace-driven simulations that simply knowing how flows relate to each other is enough for better network scheduling, meeting more deadlines, and providing higher performance isolation than what is otherwise possible using today�s application-agnostic solutions. In addition to performance improvements, coflows allow us to consolidate communication optimizations across multiple applications, simplifying software development and relieving end users from parameter tuning. On the theoretical front, we discover and characterize for the first time the concurrent open shop scheduling with coupled resources family of problems. Because any flow is also a coflow with just one flow, coflows and coflow-based solutions presented in this dissertation generalize a large body of work in both networking and scheduling literatures.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@phdthesis{mosharaf:phdthesis15,
    Title                    = {Coflow: A Networking Abstraction for Distributed Data-Parallel Applications},
    Author                   = {N. M. M. K. Chowdhury},
    School                   = {University of California, Berkeley},
    Year                     = {2015},
    Month                    = {November},
    Abstract                 = {Over the past decade, the confluence of an unprecedented growth in data volumes and the rapid rise of cloud computing has fundamentally transformed systems software and corresponding infrastructure. To deal with massive datasets, more and more applications today are scaling out to large datacenters. These distributed data-parallel applications run on tens to thousands of machines in parallel to exploit I/O parallelism, and they enable a wide variety of use cases, including interactive analysis, SQL queries, machine learning, and graph processing.

Communication between the distributed computation tasks of these applications often result in massive data transfers over the network. Consequently, concentrated efforts in both industry and academia have gone into building high-capacity, low-latency datacenter networks at scale. At the same time, researchers and practitioners have proposed a wide variety of solutions to minimize flow completion times or to ensure per-flow fairness based on the point-to-point flow abstraction that forms the basis of the TCP/IP stack.

We observe that despite rapid innovations in both applications and infrastructure, application- and network-level goals are moving further apart. Data-parallel applications care about all their flows, but today�s networks treat each point-to-point flow independently. This fundamental mismatch has resulted in complex point solutions for application developers, a myriad of configuration options for end users, and an overall loss of performance.

The key contribution of this dissertation is bridging this gap between application-level performance and network-level optimizations through the coflow abstraction. Each multipoint-to-multipoint coflow represents a collection of flows with a common application-level performance objective, enabling application-aware decision making in the network. We describe complete solutions including architectures, algorithms, and implementations that apply coflows to multiple scenarios using central coordination, and we demonstrate through large-scale cloud deployments and trace-driven simulations that simply knowing how flows relate to each other is enough for better network scheduling, meeting more deadlines, and providing higher performance isolation than what is otherwise possible using today�s application-agnostic solutions.

In addition to performance improvements, coflows allow us to consolidate communication optimizations across multiple applications, simplifying software development and relieving end users from parameter tuning. On the theoretical front, we discover and characterize for the first time the concurrent open shop scheduling with coupled resources family of problems. Because any flow is also a coflow with just one flow, coflows and coflow-based solutions presented in this dissertation generalize a large body of work in both networking and scheduling literatures.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Efficient coflow scheduling without prior knowledge
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Inter-coflow scheduling improves application-level communication performance in data-parallel clusters. However, existing efficient schedulers require a priori coflow information and ignore cluster dynamics like pipelining, task failures, and speculative executions, which limit their applicability. Schedulers without prior knowledge compromise on performance to avoid head-of-line blocking. In this paper, we present Aalo that strikes a balance and efficiently schedules coflows without prior knowledge. Aalo employs Discretized Coflow-Aware Least-Attained Service (D-CLAS) to separate coflows into a small number of priority queues based on how much they have already sent across the cluster. By performing prioritization across queues and by scheduling coflows in the FIFO order within each queue, Aalo's non-clairvoyant scheduler reduces coflow completion times while guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete 1.93X faster on average and 3.59X faster at the 95th percentile using Aalo in comparison to per-flow mechanisms. Aalo's performance is comparable to that of solutions using prior knowledge, and Aalo outperforms them in presence of cluster dynamics.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{aalo:sigcomm15,
    Title                    = {Efficient Coflow Scheduling Without Prior Knowledge},
    Author                   = {M. Chowdhury and I. Stoica},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2015},
    Month                    = {August},
    Abstract                 = {Inter-coflow scheduling improves application-level communication performance in data-parallel clusters. However, existing efficient schedulers require a priori coflow information and ignore cluster dynamics like pipelining, task failures, and speculative executions, which limit their applicability. Schedulers without prior knowledge compromise on performance to avoid head-of-line blocking. In this paper, we present Aalo that strikes a balance and efficiently schedules coflows without prior knowledge.

Aalo employs Discretized Coflow-Aware Least-Attained Service (D-CLAS) to separate coflows into a small number of priority queues based on how much they have already sent across the cluster. By performing prioritization across queues and by scheduling coflows in the FIFO order within each queue, Aalo's non-clairvoyant scheduler reduces coflow completion times while guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete 1.93X faster on average and 3.59X faster at the 95th percentile using Aalo in comparison to per-flow mechanisms. Aalo's performance is comparable to that of solutions using prior knowledge, and Aalo outperforms them in presence of cluster dynamics.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2014"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            A longitudinal and cross-dataset study of internet latency and path stability
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-R.-Agarwal ">R.&nbspAgarwal</span>, 
                            <span class="pub-author pub-author-V.-Sekar ">V.&nbspSekar</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>We present a retrospective and longitudinal study of Internet latency and path stability using three large-scale traceroute datasets collected over several years: Ark and iPlane from 2008 to 2013 and a proprietary CDN's traceroute dataset spanning 2012 and 2013. Using these different �lenses�, we revisit classical properties of Internet paths such as end-to-end latency, stability, and of routing graph structure. Iterative data analysis at this scale is challenging given the idiosyncrasies of different collection tools, measurement noise, and the diverse analysis we desire. To this end, we leverage recent big-data techniques to develop a scalable data analysis toolkit, Hummus, that enables rapid and iterative analysis on large traceroute measurement datasets. Our key findings are: (1) overall latency seems to be decreasing; (2) some geographical regions still have poor latency; (3) route stability (prevalence and persistence) is increasing; and (4) we observe a mixture of effects in the routing graph structure with high-degree ASes rapidly increasing in degree and lower-degree ASes forming denser �communities�.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@techreport{hummus:tr14,
    Title                    = {A Longitudinal and Cross-Dataset Study of Internet Latency and Path Stability},
    Author                   = {M. Chowdhury and R. Agarwal and V. Sekar and I. Stoica},
    Institution              = {UC Berkeley},
    Year                     = {2014},
    Month                    = {October},
    Number                   = {UCB/EECS-2014-172},
    Abstract                 = {We present a retrospective and longitudinal study of Internet latency and path stability using three large-scale traceroute datasets collected over several years: Ark and iPlane from 2008 to 2013 and a proprietary CDN's traceroute dataset spanning 2012 and 2013. Using these different �lenses�, we revisit classical properties of Internet paths such as end-to-end latency, stability, and of routing graph structure. Iterative data analysis at this scale is challenging given the idiosyncrasies of different collection tools, measurement noise, and the diverse analysis we desire. To this end, we leverage recent big-data techniques to develop a scalable data analysis toolkit, Hummus, that enables rapid and iterative analysis on large traceroute measurement datasets. Our key findings are: (1) overall latency seems to be decreasing; (2) some geographical regions still have poor latency; (3) route stability (prevalence and persistence) is increasing; and (4) we observe a mixture of effects in the routing graph structure with high-degree ASes rapidly increasing in degree and lower-degree ASes forming denser �communities�.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Efficient coflow scheduling with Varys
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-Y.-Zhong ">Y.&nbspZhong</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Communication in data-parallel applications often involves a collection of parallel flows. Traditional techniques to optimize flow-level metrics do not perform well in optimizing such collections, because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two different objectives: decreasing communication time of data-intensive jobs and guaranteeing predictable communication time. We introduce the concurrent open shop scheduling with coupled resources problem, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16X faster on average and up to 2X more coflows meet their deadlines using Varys in comparison to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5X.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{varys:sigcomm14,
    Title                    = {Efficient Coflow Scheduling with {Varys}},
    Author                   = {M. Chowdhury and Y. Zhong and I. Stoica},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2014},
    Month                    = {August},
    Pages                    = {443--454},
    Abstract                 = {Communication in data-parallel applications often involves a collection of parallel flows. Traditional techniques to optimize flow-level metrics do not perform well in optimizing such collections, because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two different objectives: decreasing communication time of data-intensive jobs and guaranteeing predictable communication time. We introduce the concurrent open shop scheduling with coupled resources problem, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16X faster on average and up to 2X more coflows meet their deadlines using Varys in comparison to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5X.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2013"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Leveraging endpoint flexibility in data-intensive clusters
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-S.-Kandula ">S.&nbspKandula</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Many applications do not constrain the destinations of their network transfers. New opportunities emerge when such transfers contribute a large amount of network bytes. By choosing the endpoints to avoid congested links, completion times of these transfers as well as that of others without similar flexibility can be improved. In this paper, we focus on leveraging the flexibility in replica placement during writes to cluster file systems (CFSes), which account for almost half of all cross-rack traffic in data-intensive clusters. The replicas of a CFS write can be placed in any subset of machines as long as they are in multiple fault domains and ensure a balanced use of storage throughout the cluster. We study CFS interactions with the cluster network, analyze optimizations for replica placement, and propose Sinbad - a system that identifies imbalance and adapts replica destinations to navigate around congested links. Experiments on EC2 and trace-driven simulations show that block writes complete 1.3X (respectively, 1.58X) faster as the network becomes more balanced. As a collateral benefit, end-to-end completion times of data-intensive jobs improve as well. Sinbad does so with little impact on the long-term storage balance.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{sinbad:sigcomm13,
    Title                    = {Leveraging Endpoint Flexibility in Data-Intensive Clusters},
    Author                   = {M. Chowdhury and S. Kandula and I. Stoica},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2013},
    Month                    = {August},
    Pages                    = {231--242},
    Abstract                 = {Many applications do not constrain the destinations of their network transfers. New opportunities emerge when such transfers contribute a large amount of network bytes. By choosing the endpoints to avoid congested links, completion times of these transfers as well as that of others without similar flexibility can be improved. In this paper, we focus on leveraging the flexibility in replica placement during writes to cluster file systems (CFSes), which account for almost half of all cross-rack traffic in data-intensive clusters. The replicas of a CFS write can be placed in any subset of machines as long as they are in multiple fault domains and ensure a balanced use of storage throughout the cluster.

We study CFS interactions with the cluster network, analyze optimizations for replica placement, and propose Sinbad - a system that identifies imbalance and adapts replica destinations to navigate around congested links. Experiments on EC2 and trace-driven simulations show that block writes complete 1.3X (respectively, 1.58X) faster as the network becomes more balanced. As a collateral benefit, end-to-end completion times of data-intensive jobs improve as well. Sinbad does so with little impact on the long-term storage balance.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            PolyViNE: Policy-based virtual network embedding across multiple domains
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-F.-Samuel ">F.&nbspSamuel</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Intra-domain virtual network embedding is a well-studied problem in the network virtualization literature. For most practical purposes, however, virtual networks (VNs) must be provisioned across heterogeneous administrative domains managed by multiple infrastructure providers (InPs). In this paper, we present PolyViNE, a policy-based inter-domain VN embedding framework that embeds end-to-end VNs in a decentralized manner. PolyViNE introduces a distributed protocol that coordinates the VN embedding process across participating InPs and ensures competitive prices for service providers (SPs), i.e., VN owners, while providing monetary incentives for InPs to participate in the process even under heavy competition. We also present a location-aware VN request forwarding mechanism – based on a hierarchical addressing scheme (COST) and a location awareness protocol (LAP) – to allow faster embedding and outline scalability and performance characteristics of PolyViNE through quantitative and qualitative evaluations.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{polyvine:jisa13,
    Title                    = {{PolyViNE}: Policy-based Virtual Network Embedding Across Multiple Domains},
    Author                   = {F. Samuel and M. Chowdhury and R. Boutaba},
    Journal                  = {Journal of Internet Services and Applications},
    Year                     = {2013},
    Month                    = {March},
    Number                   = {6},
    Pages                    = {23 pages},
    Volume                   = {4},
    Abstract                 = {Intra-domain virtual network embedding is a well-studied problem in the network virtualization literature. For most practical purposes, however, virtual networks (VNs) must be provisioned across heterogeneous administrative domains managed by multiple infrastructure providers (InPs). 

In this paper, we present PolyViNE, a policy-based inter-domain VN embedding framework that embeds end-to-end VNs in a decentralized manner. PolyViNE introduces a distributed protocol that coordinates the VN embedding process across participating InPs and ensures competitive prices for service providers (SPs), i.e., VN owners, while providing monetary incentives for InPs to participate in the process even under heavy competition. We also present a location-aware VN request forwarding mechanism -- based on a hierarchical addressing scheme (COST) and a location awareness protocol (LAP) -- to allow faster embedding and outline scalability and performance characteristics of PolyViNE through quantitative and qualitative evaluations.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2012"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Coflow: A networking abstraction for cluster applications
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Cluster computing applications – frameworks like MapReduce and user-facing applications like search platforms – have application-level requirements and higher-level abstractions to express them. However, there exists no networking abstraction that can take advantage of the rich semantics readily available from these data parallel applications. We propose coflow, a networking abstraction to express the communication requirements of prevalent data parallel programming paradigms. Coflows make it easier for the applications to convey their communication semantics to the network, which in turn enables the network to better optimize common communication patterns.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{coflow:hotnets12,
    Title                    = {Coflow: A Networking Abstraction for Cluster Applications},
    Author                   = {M. Chowdhury and I. Stoica},
    Booktitle                = {ACM HotNets},
    Year                     = {2012},
    Month                    = {October},
    Pages                    = {31--36},
    Abstract                 = {Cluster computing applications -- frameworks like MapReduce and user-facing applications like search platforms -- have application-level requirements and higher-level abstractions to express them. However, there exists no networking abstraction that can take advantage of the rich semantics readily available from these data parallel applications. 

We propose coflow, a networking abstraction to express the communication requirements of prevalent data parallel programming paradigms. Coflows make it easier for the applications to convey their communication semantics to the network, which in turn enables the network to better optimize common communication patterns.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Surviving failures in bandwidth-constrained datacenters
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-P.-Bodik ">P.&nbspBodik</span>, 
                            <span class="pub-author pub-author-I.-Menache ">I.&nbspMenache</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-P.-Mani ">P.&nbspMani</span>, 
                            <span class="pub-author pub-author-D.-Maltz ">D.&nbspMaltz</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Datacenter networks have been designed to tolerate failures of network equipment and provide sufficient bandwidth. In practice, however, failures and maintenance of networking and power equipment often make tens to thousands of servers unavailable, and network congestion can increase service latency. Unfortunately, there exists an inherent tradeoff between achieving high fault tolerance and reducing bandwidth usage in network core; spreading servers across fault domains improves fault tolerance, but requires additional bandwidth, while deploying servers together reduces bandwidth usage, but also decreases fault tolerance. We present a detailed analysis of a large-scale Web application and its communication patterns. Based on that, we propose and evaluate a novel optimization framework that achieves both high fault tolerance and significantly reduces bandwidth usage in the network core by exploiting the skewness in the observed communication patterns.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{ftbw:sigcomm12,
    Title                    = {Surviving Failures in Bandwidth-Constrained Datacenters},
    Author                   = {P. Bodik and I. Menache and M. Chowdhury and P. Mani and D. Maltz and I. Stoica},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2012},
    Month                    = {August},
    Pages                    = {431--442},
    Abstract                 = {Datacenter networks have been designed to tolerate failures of network equipment and provide sufficient bandwidth. In practice, however, failures and maintenance of networking and power equipment often make tens to thousands of servers unavailable, and network congestion can increase service latency. Unfortunately, there exists an inherent tradeoff between achieving high fault tolerance and reducing bandwidth usage in network core; spreading servers across fault domains improves fault tolerance, but requires additional bandwidth, while deploying servers together reduces bandwidth usage, but also decreases fault tolerance. We present a detailed analysis of a large-scale Web application and its communication patterns. Based on that, we propose and evaluate a novel optimization framework that achieves both high fault tolerance and significantly reduces bandwidth usage in the network core by exploiting the skewness in the observed communication patterns.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Coflow: An application layer abstraction for cluster networking
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Cluster computing applications, whether frameworks like MapReduce and Dryad, or customized applications like search platforms and social networks, have application-level requirements and higher-level abstractions to express them. Networking, however, still remains at the level of forwarding packets and balancing flows, and there exists no networking abstraction that can take advantage of the rich semantics readily available from these data parallel applications. The result is a plethora of seemingly disjoint, yet somehow connected, pieces of work to address networking challenges in these applications. We propose an application layer, data plane abstraction, coflow, that can express the requirements of (data) parallel programming models used in clusters today and makes it easier to express, reason about, and act upon these requirements.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@techreport{coflow:tr12,
    Title                    = {Coflow: An Application Layer Abstraction for Cluster Networking},
    Author                   = {M. Chowdhury and I. Stoica},
    Institution              = {UC Berkeley},
    Year                     = {2012},
    Month                    = {August},
    Number                   = {UCB/EECS-2012-184},
    Abstract                 = {Cluster computing applications, whether frameworks like MapReduce and Dryad, or customized applications like search platforms and social networks, have application-level requirements and higher-level abstractions to express them. Networking, however, still remains at the level of forwarding packets and balancing flows, and there exists no networking abstraction that can take advantage of the rich semantics readily available from these data parallel applications. The result is a plethora of seemingly disjoint, yet somehow connected, pieces of work to address networking challenges in these applications.

We propose an application layer, data plane abstraction, coflow, that can express the requirements of (data) parallel programming models used in clusters today and makes it easier to express, reason about, and act upon these requirements.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            FairCloud: Sharing the network in cloud computing
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-L.-Popa ">L.&nbspPopa</span>, 
                            <span class="pub-author pub-author-G.-Kumar ">G.&nbspKumar</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-A.-Krishnamurthy ">A.&nbspKrishnamurthy</span>, 
                            <span class="pub-author pub-author-S.-Ratnasamy ">S.&nbspRatnasamy</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>The network, similar to CPU and memory, is a critical and shared resource in the cloud. However, unlike other resources, it is neither shared proportionally to payment, nor do cloud providers offer minimum guarantees on network bandwidth. The reason networks are more difficult to share is because the network allocation of a virtual machine (VM) X depends not only on the VMs running on the same machine with X, but also on the other VMs that X communicates with and the cross-traffic on each link used by X. In this paper, we start from the above requirements–payment proportionality and minimum guarantees–and show that the network-specific challenges lead to fundamental tradeoffs when sharing cloud networks. We then propose a set of properties to explicitly express these tradeoffs. Finally, we present three allocation policies that allow us to navigate the tradeoff space. We evaluate their characteristics through simulation and testbed experiments to show that they can provide minimum guarantees and achieve better proportionality than existing solutions.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{faircloud:sigcomm12,
    Title                    = {{FairCloud}: Sharing The Network in Cloud Computing},
    Author                   = {L. Popa and G. Kumar and M. Chowdhury and A. Krishnamurthy and S. Ratnasamy and I. Stoica},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2012},
    Month                    = {August},
    Pages                    = {187--198},
    Abstract                 = {The network, similar to CPU and memory, is a critical and shared resource in the cloud. However, unlike other resources, it is neither shared proportionally to payment, nor do cloud providers offer minimum guarantees on network bandwidth. The reason networks are more difficult to share is because the network allocation of a virtual machine (VM) X depends not only on the VMs running on the same machine with X, but also on the other VMs that X communicates with and the cross-traffic on each link used by X. In this paper, we start from the above requirements--payment proportionality and minimum guarantees--and show that the network-specific challenges lead to fundamental tradeoffs when sharing cloud networks. We then propose a set of properties to explicitly express these tradeoffs. Finally, we present three allocation policies that allow us to navigate the tradeoff space. We evaluate their characteristics through simulation and testbed experiments to show that they can provide minimum guarantees and achieve better proportionality than existing solutions.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Fast and interactive analytics over Hadoop data with Spark
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Zaharia ">M.&nbspZaharia</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-T.-Das ">T.&nbspDas</span>, 
                            <span class="pub-author pub-author-A.-Dave ">A.&nbspDave</span>, 
                            <span class="pub-author pub-author-J.-Ma ">J.&nbspMa</span>, 
                            <span class="pub-author pub-author-M.-McCauley ">M.&nbspMcCauley</span>, 
                            <span class="pub-author pub-author-M.-J. Franklin ">M.&nbspJ.&nbspFranklin</span>, 
                            <span class="pub-author pub-author-S.-Shenker ">S.&nbspShenker</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>The past few years have seen tremendous interest in large-scale data analysis, as data volumes in both industry and research continue to outgrow the processing speed of individual machines. Google's MapReduce model and its open source implementation, Hadoop, kicked off an ecosystem of parallel data analysis tools for large clusters, such as Apache's Hive and Pig engines for SQL processing. However, these tools have so far been optimized for one-pass batch processing of on-disk data, which makes them slow for interactive data exploration and for the more complex multi-pass analytics algorithms that are becoming common. In this article, we introduce Spark, a new cluster computing framework that can run applications up to 40X faster than Hadoop by keeping data in memory, and can be used interactively to query large datasets with sub-second latency.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{spark:login12,
    Title                    = {Fast and Interactive Analytics over {Hadoop} Data with {Spark}},
    Author                   = {M. Zaharia and M. Chowdhury and T. Das and A. Dave and J. Ma and M. McCauley and M. J. Franklin and S. Shenker and I. Stoica},
    Journal                  = {USENIX ;login:},
    Year                     = {2012},
    Month                    = {August},
    Number                   = {4},
    Pages                    = {45--51},
    Volume                   = {37},
    Abstract                 = {The past few years have seen tremendous interest in large-scale data analysis, as data volumes in both industry and research continue to outgrow the processing speed of individual machines. Google's MapReduce model and its open source implementation, Hadoop, kicked off an ecosystem of parallel data analysis tools for large clusters, such as Apache's Hive and Pig engines for SQL processing. However, these tools have so far been optimized for one-pass batch processing of on-disk data, which makes them slow for interactive data exploration and for the more complex multi-pass analytics algorithms that are becoming common. In this article, we introduce Spark, a new cluster computing framework that can run applications up to 40X faster than Hadoop by keeping data in memory, and can be used interactively to query large datasets with sub-second latency.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            A case for performance-centric network allocation
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-G.-Kumar ">G.&nbspKumar</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-S.-Ratnasamy ">S.&nbspRatnasamy</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>We consider the problem of allocating network resources across applications in a private cluster running data-parallel frameworks. Our primary observation is that these applications have different communication requirements and thus require different support from the network to effectively parallelize. We argue that network resources should be shared in a performance-centric fashion that aids parallelism and allows developers to reason about the overall performance of their applications. This paper tries to address the question of whether/how fairness-centric proposals relate to a performance-centric approach for different communication patterns common in these frameworks and engages in a quest for a unified mechanism to share the network in such settings.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{ahimsa:hotcloud12,
    Title                    = {A Case for Performance-Centric Network Allocation},
    Author                   = {G. Kumar and M. Chowdhury and S. Ratnasamy and I. Stoica},
    Booktitle                = {USENIX HotCloud},
    Year                     = {2012},
    Month                    = {June},
    Pages                    = {6 pages},
    Abstract                 = {We consider the problem of allocating network resources across applications in a private cluster running data-parallel frameworks. Our primary observation is that these applications have different communication requirements and thus require different support from the network to effectively parallelize. We argue that network resources should be shared in a performance-centric fashion that aids parallelism and allows developers to reason about the overall performance of their applications. This paper tries to address the question of whether/how fairness-centric proposals relate to a performance-centric approach for different communication patterns common in these frameworks and engages in a quest for a unified mechanism to share the network in such settings.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Zaharia ">M.&nbspZaharia</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-T.-Das ">T.&nbspDas</span>, 
                            <span class="pub-author pub-author-A.-Dave ">A.&nbspDave</span>, 
                            <span class="pub-author pub-author-J.-Ma ">J.&nbspMa</span>, 
                            <span class="pub-author pub-author-M.-McCauley ">M.&nbspMcCauley</span>, 
                            <span class="pub-author pub-author-M.-J. Franklin ">M.&nbspJ.&nbspFranklin</span>, 
                            <span class="pub-author pub-author-S.-Shenker ">S.&nbspShenker</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{spark:nsdi12,
    Title                    = {Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing},
    Author                   = {M. Zaharia and M. Chowdhury and T. Das and A. Dave and J. Ma and M. McCauley and M. J. Franklin and S. Shenker and I. Stoica},
    Booktitle                = {USENIX NSDI},
    Year                     = {2012},
    Month                    = {April},
    Pages                    = {14 pages},
    Abstract                 = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            ViNEYard: Virtual network embedding algorithms with coordinated node and link mapping
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-M.-R. Rahman ">M.&nbspR.&nbspRahman</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Network virtualization allows multiple heterogeneous virtual networks (VNs) to coexist on a shared infrastructure. Efficient mapping of virtual nodes and virtual links of a VN request onto substrate network resources, also known as the VN embedding problem, is the first step toward enabling such multiplicity. Since this problem is known to be NP-hard, previous research focused on designing heuristic-based algorithms that had clear separation between the node mapping and the link mapping phases. In this paper, we present ViNEYard – a collection of VN embedding algorithms that leverage better coordination between the two phases. We formulate the VN embedding problem as a mixed integer program through substrate network augmentation. We then relax the integer constraints to obtain a linear program and devise two online VN embedding algorithms D-ViNE and R-ViNE using deterministic and randomized rounding techniques, respectively. We also present a generalized window-based VN embedding algorithm (WiNE) to evaluate the effect of lookahead on VN embedding. Our simulation experiments on a large mix of VN requests show that the proposed algorithms increase the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network in the long run.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{vineyard:ton12,
    Title                    = {{ViNEYard}: Virtual Network Embedding Algorithms with Coordinated Node and Link Mapping},
    Author                   = {M. Chowdhury and M. R. Rahman and R. Boutaba},
    Journal                  = {IEEE/ACM Transactions on Networking},
    Year                     = {2012},
    Month                    = {February},
    Number                   = {1},
    Pages                    = {206--219},
    Volume                   = {20},
    Abstract                 = {Network virtualization allows multiple heterogeneous virtual networks (VNs) to coexist on a shared infrastructure. Efficient mapping of virtual nodes and virtual links of a VN request onto substrate network resources, also known as the VN embedding problem, is the first step toward enabling such multiplicity. Since this problem is known to be NP-hard, previous research focused on designing heuristic-based algorithms that had clear separation between the node mapping and the link mapping phases.

In this paper, we present ViNEYard -- a collection of VN embedding algorithms that leverage better coordination between the two phases. We formulate the VN embedding problem as a mixed integer program through substrate network augmentation. We then relax the integer constraints to obtain a linear program and devise two online VN embedding algorithms D-ViNE and R-ViNE using deterministic and randomized rounding techniques, respectively. We also present a generalized window-based VN embedding algorithm (WiNE) to evaluate the effect of lookahead on VN embedding. Our simulation experiments on a large mix of VN requests show that the proposed algorithms increase the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network in the long run.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2011"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Lattice: A scalable layer-agnostic packet classification framework
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-S.-Agarwal ">S.&nbspAgarwal</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-D.-Joseph ">D.&nbspJoseph</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Despite widespread application, packet classification is implemented and deployed in an ad-hoc manner at different layers of the protocol stack. Moreover, high speed packet classification, in presence of a large number of classification rules, is both resource and computation intensive. We propose a scalable layer-agnostic packet classification framework (Lattice) that generalizes classifier design and enables offloading part of computation and memory requirements to helpers (e.g., end hosts). Lattice eliminates per-packet classification and per-flow states in classifiers to increase scalability and decreases vulnerability to state-based DoS attacks. Furthermore, Lattice is incentive compatible in that helpers cannot get better service by lying, and it incentivizes deployment by giving preferential treatment to packets carrying Lattice-related information. Finally, Lattice-enabled classifiers remain semantically equivalent to their unmodified counterparts. To evaluate Lattice, we have built a prototype using the Click software router and implemented multiple Lattice-enabled classifiers. Lattice-enabled firewalls perform at least 2X faster than unmodified counterparts and scale well with the increasing number of classification rules.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@techreport{lattice:tr11,
    Title                    = {Lattice: A Scalable Layer-Agnostic Packet Classification Framework},
    Author                   = {S. Agarwal and M. Chowdhury and D. Joseph and I. Stoica},
    Institution              = {UC Berkeley},
    Year                     = {2011},
    Month                    = {August},
    Number                   = {UCB/EECS-2011-96},
    Abstract                 = {Despite widespread application, packet classification is implemented and deployed in an ad-hoc manner at different layers of the protocol stack. Moreover, high speed packet classification, in presence of a large number of classification rules, is both resource and computation intensive. We propose a scalable layer-agnostic packet classification framework (Lattice) that generalizes classifier design and enables offloading part of computation and memory requirements to helpers (e.g., end hosts). Lattice eliminates per-packet classification and per-flow states in classifiers to increase scalability and decreases vulnerability to state-based DoS attacks. Furthermore, Lattice is incentive compatible in that helpers cannot get better service by lying, and it incentivizes deployment by giving preferential treatment to packets carrying Lattice-related information. Finally, Lattice-enabled classifiers remain semantically equivalent to their unmodified counterparts. To evaluate Lattice, we have built a prototype using the Click software router and implemented multiple Lattice-enabled classifiers. Lattice-enabled firewalls perform at least 2X faster than unmodified counterparts and scale well with the increasing number of classification rules.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Managing data transfers in computer clusters with Orchestra
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-M.-Zaharia ">M.&nbspZaharia</span>, 
                            <span class="pub-author pub-author-J.-Ma ">J.&nbspMa</span>, 
                            <span class="pub-author pub-author-M.-I. Jordan ">M.&nbspI.&nbspJordan</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Cluster computing applications like MapReduce and Dryad transfer massive amounts of data between their computation stages. These transfers can have a significant impact on job performance, accounting for more than 50% of job completion times. Despite this impact, there has been relatively little work on optimizing the performance of these data transfers, with networking researchers traditionally focusing on per-flow traffic management. We address this limitation by proposing a global management architecture and a set of algorithms that (1) improve the transfer times of common communication patterns, such as broadcast and shuffle, and (2) allow scheduling policies at the transfer level, such as prioritizing a transfer over other transfers. Using a prototype implementation, we show that our solution improves broadcast completion times by up to 4.5X compared to the status quo in Hadoop. We also show that transfer-level scheduling can reduce the completion time of high-priority transfers by 1.7X.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{orchestra:sigcomm11,
    Title                    = {Managing Data Transfers in Computer Clusters with {Orchestra}},
    Author                   = {M. Chowdhury and M. Zaharia and J. Ma and M. I. Jordan and I. Stoica},
    Booktitle                = {ACM SIGCOMM},
    Year                     = {2011},
    Month                    = {August},
    Pages                    = {98--109},
    Abstract                 = {Cluster computing applications like MapReduce and Dryad transfer massive amounts of data between their computation stages. These transfers can have a significant impact on job performance, accounting for more than 50% of job completion times. Despite this impact, there has been relatively little work on optimizing the performance of these data transfers, with networking researchers traditionally focusing on per-flow traffic management. We address this limitation by proposing a global management architecture and a set of algorithms that (1) improve the transfer times of common communication patterns, such as broadcast and shuffle, and (2) allow scheduling policies at the transfer level, such as prioritizing a transfer over other transfers. Using a prototype implementation, we show that our solution improves broadcast completion times by up to 4.5X compared to the status quo in Hadoop. We also show that transfer-level scheduling can reduce the completion time of high-priority transfers by 1.7X.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Zaharia ">M.&nbspZaharia</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-T.-Das ">T.&nbspDas</span>, 
                            <span class="pub-author pub-author-A.-Dave ">A.&nbspDave</span>, 
                            <span class="pub-author pub-author-J.-Ma ">J.&nbspMa</span>, 
                            <span class="pub-author pub-author-M.-McCauley ">M.&nbspMcCauley</span>, 
                            <span class="pub-author pub-author-M.-J. Franklin ">M.&nbspJ.&nbspFranklin</span>, 
                            <span class="pub-author pub-author-S.-Shenker ">S.&nbspShenker</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that allows programmers to perform in-memory computations on large clusters while retaining the fault tolerance of data flow models like MapReduce. RDDs are motivated by two types of applications that current data flow systems handle inefficiently: iterative algorithms, which are common in graph applications and machine learning, and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a highly restricted form of shared memory: they are read-only datasets that can only be constructed through bulk operations on other RDDs. However, we show that RDDs are expressive enough to capture a wide class of computations, including MapReduce and specialized programming models for iterative jobs such as Pregel. Our implementation of RDDs can outperform Hadoop by 20x for iterative jobs and can be used interactively to search a 1 TB dataset with latencies of 5-7 seconds.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@techreport{spark:tr11,
    Title                    = {Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing},
    Author                   = {M. Zaharia and M. Chowdhury and T. Das and A. Dave and J. Ma and M. McCauley and M. J. Franklin and S. Shenker and I. Stoica},
    Institution              = {UC Berkeley},
    Year                     = {2011},
    Month                    = {July},
    Number                   = {UCB/EECS-2011-82},
    Abstract                 = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that allows programmers to perform in-memory computations on large clusters while retaining the fault tolerance of data flow models like MapReduce. RDDs are motivated by two types of applications that current data flow systems handle inefficiently: iterative algorithms, which are common in graph applications and machine learning, and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a highly restricted form of shared memory: they are read-only datasets that can only be constructed through bulk operations on other RDDs. However, we show that RDDs are expressive enough to capture a wide class of computations, including MapReduce and specialized programming models for iterative jobs such as Pregel. Our implementation of RDDs can outperform Hadoop by 20x for iterative jobs and can be used interactively to search a 1 TB dataset with latencies of 5-7 seconds.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2010"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            PolyViNE: Policy-based virtual network embedding across multiple domains
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-F.-Samuel ">F.&nbspSamuel</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Intra-domain virtual network embedding (ViNE) is a well studied problem in the network virtualization literature. For most practical purposes, however, virtual networks (VNs) must be provisioned across heterogeneous administrative domains managed by multiple infrastructure providers (InPs). In this paper we present PolyViNE, a policy-based inter-domain VN embedding framework that embeds end-to-end VNs in a decentralized manner. PolyViNE introduces a distributed protocol that coordinates the VN embedding process across participating InPs and ensures competitive prices for service providers (SPs), i.e., VN owners. We also present a location aware VN request forwarding mechanism – based on a hierarchical addressing scheme (COST) and a location awareness protocol (LAP) – to allow faster embedding and outline scalability and performance characteristics of PolyViNE through quantitative and qualitative evaluations.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{polyvine:visa10,
    Title                    = {{PolyViNE}: Policy-based Virtual Network Embedding Across Multiple Domains},
    Author                   = {M. Chowdhury and F. Samuel and R. Boutaba},
    Booktitle                = {ACM SIGCOMM VISA},
    Year                     = {2010},
    Month                    = {September},
    Pages                    = {49--56},
    Abstract                 = {Intra-domain virtual network embedding (ViNE) is a well studied problem in the network virtualization literature. For most practical purposes, however, virtual networks (VNs) must be provisioned across heterogeneous administrative domains managed by multiple infrastructure providers (InPs). 

In this paper we present PolyViNE, a policy-based inter-domain VN embedding framework that embeds end-to-end VNs in a decentralized manner. PolyViNE introduces a distributed protocol that coordinates the VN embedding process across participating InPs and ensures competitive prices for service providers (SPs), i.e., VN owners. We also present a location aware VN request forwarding mechanism -- based on a hierarchical addressing scheme (COST) and a location awareness protocol (LAP) -- to allow faster embedding and outline scalability and performance characteristics of PolyViNE through quantitative and qualitative evaluations.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Spark: Cluster computing with working sets
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Zaharia ">M.&nbspZaharia</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-M.-J. Franklin ">M.&nbspJ.&nbspFranklin</span>, 
                            <span class="pub-author pub-author-S.-Shenker ">S.&nbspShenker</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{spark:hotcloud10,
    Title                    = {Spark: Cluster Computing with Working Sets},
    Author                   = {M. Zaharia and M. Chowdhury and M. J. Franklin and S. Shenker and I. Stoica},
    Booktitle                = {USENIX HotCloud},
    Year                     = {2010},
    Month                    = {June},
    Pages                    = {6 pages},
    Abstract                 = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Topology-awareness and reoptimization mechanism for virtual network embedding
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-F. Butt ">N.&nbspF.&nbspButt</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Embedding of virtual network (VN) requests on top of a shared physical network poses an intriguing combination of theoretical and practical challenges. Two major problems with the state-of-the-art VN embedding algorithms are their indifference to the underlying substrate topology and their lack of reoptimization mechanisms for already embedded VN requests. We argue that topology-aware embedding together with reoptimization mechanisms can ameliorate the performance of the previous VN embedding algorithms in terms of acceptance ratio and load balancing. The major contributions of this paper are twofold: (1) we present a mechanism to differentiate among resources based on their importance in the substrate topology, and (2) we propose a set of algorithms for reoptimizing and re-embedding initially-rejected VN requests after fixing their bottleneck requirements. Through extensive simulations, we show that not only our techniques improve the acceptance ratio, but they also provide the added benefit of balancing load better than the previous proposals.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{tarmvine:networking10,
    Title                    = {Topology-Awareness and Reoptimization Mechanism for Virtual Network Embedding},
    Author                   = {N. F. Butt and M. Chowdhury and R. Boutaba},
    Booktitle                = {IFIP NETWORKING},
    Year                     = {2010},
    Month                    = {May},
    Pages                    = {27--39},
    Abstract                 = {Embedding of virtual network (VN) requests on top of a shared physical network poses an intriguing combination of theoretical and practical challenges. Two major problems with the state-of-the-art VN embedding algorithms are their indifference to the underlying substrate topology and their lack of reoptimization mechanisms for already embedded VN requests. We argue that topology-aware embedding together with reoptimization mechanisms can ameliorate the performance of the previous VN embedding algorithms in terms of acceptance ratio and load balancing. The major contributions of this paper are twofold: (1) we present a mechanism to differentiate among resources based on their importance in the substrate topology, and (2) we propose a set of algorithms for reoptimizing and re-embedding initially-rejected VN requests after fixing their bottleneck requirements. Through extensive simulations, we show that not only our techniques improve the acceptance ratio, but they also provide the added benefit of balancing load better than the previous proposals.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Spark: Cluster computing with working sets
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-M.-Zaharia ">M.&nbspZaharia</span>, 
                            <span class="pub-author pub-author-M.-Chowdhury ">M.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-M.-J. Franklin ">M.&nbspJ.&nbspFranklin</span>, 
                            <span class="pub-author pub-author-S.-Shenker ">S.&nbspShenker</span>, and 
                            <span class="pub-author pub-author-I.-Stoica ">I.&nbspStoica</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>MapReduce and its variants have been highly successful in implementing large-scale data intensive applications on clusters of unreliable machines. However, most of these systems are built around an acyclic data flow programming model that is not suitable for other popular applications. In this paper, we focus on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis environments. We propose a new framework called Spark that supports these applications while maintaining the scalability and fault-tolerance properties of MapReduce. To achieve these goals, Spark introduces a data abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@techreport{spark:tr10,
    Title                    = {Spark: Cluster Computing with Working Sets},
    Author                   = {M. Zaharia and M. Chowdhury and M. J. Franklin and S. Shenker and I. Stoica},
    Institution              = {UC Berkeley},
    Year                     = {2010},
    Month                    = {May},
    Number                   = {UCB/EECS-2010-53},
    Abstract                 = {MapReduce and its variants have been highly successful in implementing large-scale data intensive applications on clusters of unreliable machines. However, most of these systems are built around an acyclic data flow programming model that is not suitable for other popular applications. In this paper, we focus on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis environments. We propose a new framework called Spark that supports these applications while maintaining the scalability and fault-tolerance properties of MapReduce. To achieve these goals, Spark introduces a data abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            A survey of network virtualization
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Due to the existence of multiple stakeholders with conflicting goals and policies, alterations to the existing Internet architecture are now limited to simple incremental updates; deployment of any new, radically different technology is next to impossible. To fend off this ossification, network virtualization has been propounded as a diversifying attribute of the future inter-networking paradigm. By introducing a plurality of heterogeneous network architectures cohabiting on a shared physical substrate, network virtualization promotes innovations and diversified applications. In this paper, we survey the existing technologies and a wide array of past and state-of-the-art projects on network virtualization followed by a discussion of major challenges in this area.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{nv-survey:comnet10,
    Title                    = {A Survey of Network Virtualization},
    Author                   = {N. M. M. K. Chowdhury and R. Boutaba},
    Journal                  = {Computer Networks},
    Year                     = {2010},
    Month                    = {April},
    Number                   = {5},
    Pages                    = {862--876},
    Volume                   = {54},
    Abstract                 = {Due to the existence of multiple stakeholders with conflicting goals and policies, alterations to the existing Internet architecture are now limited to simple incremental updates; deployment of any new, radically different technology is next to impossible. To fend off this ossification, network virtualization has been propounded as a diversifying attribute of the future inter-networking paradigm. By introducing a plurality of heterogeneous network architectures cohabiting on a shared physical substrate, network virtualization promotes innovations and diversified applications. In this paper, we survey the existing technologies and a wide array of past and state-of-the-art projects on network virtualization followed by a discussion of major challenges in this area.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2009"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Network virtualization: State of the art and research challenges
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Recently network virtualization has been pushed forward by its proponents as a long term solution to the gradual ossification problem faced by the existing Internet and proposed to be an integral part of the next-generation networking paradigm. By allowing multiple heterogeneous network architectures to cohabit on a shared physical substrate, network virtualization provides flexibility, promotes diversity, and promises security and increased manageability. However, many technical issues stand in the way toward its successful realization. This article investigates the past and the state of the art in network virtualization along with the future challenges that must be addressed to realize a viable network virtualization environment.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{nv-overview:commag09,
    Title                    = {Network Virtualization: State of the Art and Research Challenges},
    Author                   = {N. M. M. K. Chowdhury and R. Boutaba},
    Journal                  = {IEEE Communications Magazine},
    Year                     = {2009},
    Month                    = {July},
    Number                   = {7},
    Pages                    = {20--26},
    Volume                   = {47},
    Abstract                 = {Recently network virtualization has been pushed forward by its proponents as a long term solution to the gradual ossification problem faced by the existing Internet and proposed to be an integral part of the next-generation networking paradigm. By allowing multiple heterogeneous network architectures to cohabit on a shared physical substrate, network virtualization provides flexibility, promotes diversity, and promises security and increased manageability. However, many technical issues stand in the way toward its successful realization. This article investigates the past and the state of the art in network virtualization along with the future challenges that must be addressed to realize a viable network virtualization environment.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            iMark: An identity management framework for network virtualization environment
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-F.-E-Zaheer ">F.-E&nbspZaheer</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>In recent years, network virtualization has been propounded as an open and flexible future internetworking paradigm that allows multiple virtual networks (VNs) to coexist on a shared physical substrate. Each VN in a network virtualization environment (NVE) is free to implement its own naming, addressing, routing, and transport mechanisms. While such flexibility allows fast and easy deployment of diversified applications and services, ensuring end-to-end communication and universal connectivity poses a daunting challenge. This paper advocates that effective and efficient management of heterogeneous identifier spaces is the key to solving the problem of end-to-end connectivity in an NVE. We propose iMark, an identity management framework based on a global identity space, which enables end hosts to communicate with each other within and outside of their own networks through a set of controllers, adapters, and well-placed mappings without sacrificing the autonomy of the concerned VNs. We describe the procedures that manipulate these mappings between different identifier spaces and provide performance evaluation of the proposed framework.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{imark:im09,
    Title                    = {iMark: An Identity Management Framework for Network Virtualization Environment},
    Author                   = {N. M. M. K. Chowdhury and F.-E Zaheer and R. Boutaba},
    Booktitle                = {IFIP/IEEE IM},
    Year                     = {2009},
    Month                    = {June},
    Pages                    = {335--342},
    Abstract                 = {In recent years, network virtualization has been propounded as an open and flexible future internetworking paradigm that allows multiple virtual networks (VNs) to coexist on a shared physical substrate. Each VN in a network virtualization environment (NVE) is free to implement its own naming, addressing, routing, and transport mechanisms. While such flexibility allows fast and easy deployment of diversified applications and services, ensuring end-to-end communication and universal connectivity poses a daunting challenge. 

This paper advocates that effective and efficient management of heterogeneous identifier spaces is the key to solving the problem of end-to-end connectivity in an NVE. We propose iMark, an identity management framework based on a global identity space, which enables end hosts to communicate with each other within and outside of their own networks through a set of controllers, adapters, and well-placed mappings without sacrificing the autonomy of the concerned VNs. We describe the procedures that manipulate these mappings between different identifier spaces and provide performance evaluation of the proposed framework.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Virtual network embedding with coordinated node and link mapping
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-M.-R. Rahman ">M.&nbspR.&nbspRahman</span>, and 
                            <span class="pub-author pub-author-R.-Boutaba ">R.&nbspBoutaba</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Recently network virtualization has been proposed as a promising way to overcome the current ossification of the Internet by allowing multiple heterogeneous virtual networks (VNs) to coexist on a shared infrastructure. A major challenge in this respect is the VN embedding problem that deals with efficient mapping of virtual nodes and virtual links onto the substrate network resources. Since this problem is known to be NP-hard, previous research focused on designing heuristic-based algorithms which had clear separation between the node mapping and the link mapping phases. This paper proposes VN embedding algorithms with better coordination between the two phases. We formulate the VN embedding problem as a mixed integer program through substrate network augmentation. We then relax the integer constraints to obtain a linear program, and devise two VN embedding algorithms D-ViNE and R-ViNE using deterministic and randomized rounding techniques, respectively. Simulation experiments show that the proposed algorithms increase the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network in the long run.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{vineyard:infocom09,
    Title                    = {Virtual Network Embedding with Coordinated Node and Link Mapping},
    Author                   = {N. M. M. K. Chowdhury and M. R. Rahman and R. Boutaba},
    Booktitle                = {IEEE INFOCOM},
    Year                     = {2009},
    Month                    = {April},
    Pages                    = {783--791},
    Abstract                 = {Recently network virtualization has been proposed as a promising way to overcome the current ossification of the Internet by allowing multiple heterogeneous virtual networks (VNs) to coexist on a shared infrastructure. A major challenge in this respect is the VN embedding problem that deals with efficient mapping of virtual nodes and virtual links onto the substrate network resources. Since this problem is known to be NP-hard, previous research focused on designing heuristic-based algorithms which had clear separation between the node mapping and the link mapping phases.

This paper proposes VN embedding algorithms with better coordination between the two phases. We formulate the VN embedding problem as a mixed integer program through substrate network augmentation. We then relax the integer constraints to obtain a linear program, and devise two VN embedding algorithms D-ViNE and R-ViNE using deterministic and randomized rounding techniques, respectively. Simulation experiments show that the proposed algorithms increase the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network in the long run.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Identity management and resource allocation in the network virtualization environment
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>Due to the existence of multiple stakeholders with conflicting goals and policies, alterations to the existing Internet architecture are now limited to simple incremental updates; deployment of any new, radically different technology is next to impossible. To fend off this ossification, network virtualization has been propounded as a diversifying attribute of the future inter-networking paradigm. However, many technical issues stand in the way toward its successful realization. In this thesis, we address two basic problems in the network virtualization environment. The identity management problem is primarily concerned with ensuring interoperability across heterogeneous identifier spaces for locating and identifying end hosts in different virtual networks. We propose a novel identity management framework (iMark) that enables end-to-end connectivity across heterogeneous virtual networks without revoking their autonomy. We describe the architectural and the functional components of iMark accompanied by the procedures that manipulate these components and validate it through numerical evaluation. The virtual network embedding problem deals with the mapping of virtual nodes and links onto physical network resources. We argue that the separation of the node mapping and the link mapping phases in the existing algorithms considerably reduces the solution space and degrades embedding quality. We propose coordinated node and link mapping, based on a mathematical programming formulation, to devise two algorithms (D-ViNE and R-ViNE) for the online version of the problem under realistic assumptions. Extensive simulation experiments show that the proposed algorithms significantly outperform the existing heuristics by increasing the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@mastersthesis{nmmkchow:msthesis09,
    Title                    = {Identity Management and Resource Allocation in the Network Virtualization Environment},
    Author                   = {N. M. M. K. Chowdhury},
    School                   = {University of Waterloo},
    Year                     = {2009},
    Month                    = {January},
    Abstract                 = {Due to the existence of multiple stakeholders with conflicting goals and policies, alterations to the existing Internet architecture are now limited to simple incremental updates; deployment of any new, radically different technology is next to impossible. To fend off this ossification, network virtualization has been propounded as a diversifying attribute of the future inter-networking paradigm. However, many technical issues stand in the way toward its successful realization. In this thesis, we address two basic problems in the network virtualization environment.

The identity management problem is primarily concerned with ensuring interoperability across heterogeneous identifier spaces for locating and identifying end hosts in different virtual networks. We propose a novel identity management framework (iMark) that enables end-to-end connectivity across heterogeneous virtual networks without revoking their autonomy. We describe the architectural and the functional components of iMark accompanied by the procedures that manipulate these components and validate it through numerical evaluation. 

The virtual network embedding problem deals with the mapping of virtual nodes and links onto physical network resources. We argue that the separation of the node mapping and the link mapping phases in the existing algorithms considerably reduces the solution space and degrades embedding quality. We propose coordinated node and link mapping, based on a mathematical programming formulation, to devise two algorithms (D-ViNE and R-ViNE) for the online version of the problem under realistic assumptions. Extensive simulation experiments show that the proposed algorithms significantly outperform the existing heuristics by increasing the acceptance ratio and the revenue while decreasing the cost incurred by the substrate network.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2007"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            Admission control algorithm for multimedia server: A hybrid approach
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-D.-T. Ahmed ">D.&nbspT.&nbspAhmed</span>, 
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>, and 
                            <span class="pub-author pub-author-M.-M. Akbar ">M.&nbspM.&nbspAkbar</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>A multimedia server employs admission control algorithm (ACA) to control client traffic in order to increase utilization of server resources. The admission process accepts new clients as long as it does not violate service requirements of pre-existing clients. In this paper, we are proposing a hybrid admission control algorithm (HACA) that can handle a considerably large number of clients simultaneously by engaging different admission policies for different clients based on their service requirements. The performance of an admission control algorithm is dependent on the disk-scheduling algorithm it uses. In this paper we consider various disk-scheduling algorithms to measure the performance of our proposed algorithm. We also introduce some techniques for minimizing overflow of rounds that significantly improve the performance and demonstrate the effectiveness of HACA.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@article{haca:ijca07,
    Title                    = {Admission Control Algorithm for Multimedia Server: A Hybrid Approach},
    Author                   = {D. T. Ahmed and N. M. M. K. Chowdhury and M. M. Akbar},
    Journal                  = {International Journal of Computers and Applications},
    Year                     = {2007},
    Month                    = {December},
    Number                   = {4},
    Pages                    = {414--419},
    Volume                   = {29},
    Abstract                 = {A multimedia server employs admission control algorithm (ACA) to control client traffic in order to increase utilization of server resources. The admission process accepts new clients as long as it does not violate service requirements of pre-existing clients. In this paper, we are proposing a hybrid admission control algorithm (HACA) that can handle a considerably large number of clients simultaneously by engaging different admission policies for different clients based on their service requirements. The performance of an admission control algorithm is dependent on the disk-scheduling algorithm it uses. In this paper we consider various disk-scheduling algorithms to measure the performance of our proposed algorithm. We also introduce some techniques for minimizing overflow of rounds that significantly improve the performance and demonstrate the effectiveness of HACA.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            DiskTrie: An efficient data structure using flash memory for mobile devices
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>, 
                            <span class="pub-author pub-author-M.-M. Akbar ">M.&nbspM.&nbspAkbar</span>, and 
                            <span class="pub-author pub-author-M.-Kaykobad ">M.&nbspKaykobad</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote><p>DiskTrie is an efficient external-memory data structure for storing strings in mobile devices using flash memory. It supports Lookup and Prefix-Matching operations with a constant internal memory and linear processing requirements. The number of disk accesses it takes to search for a string among <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> unique finite strings is bounded by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><msup><mo><mi>log</mi><mo>⁡</mo></mo><mo lspace="0em" rspace="0em">∗</mo></msup><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Theta(\log^{*} n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0230359999999998em;vertical-align:-0.25em;"></span><span class="mord">Θ</span><span class="mopen">(</span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.773036em;"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>, while for a prefix-matching operation it takes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><msup><mo><mi>log</mi><mo>⁡</mo></mo><mo lspace="0em" rspace="0em">∗</mo></msup><mi>n</mi><mo stretchy="false">)</mo><mo>+</mo><mi>O</mi><mo stretchy="false">(</mo><mfrac><mi>n</mi><mi>B</mi></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Theta(\log^{*} n)+O(\frac{n}{B})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0230359999999998em;vertical-align:-0.25em;"></span><span class="mord">Θ</span><span class="mopen">(</span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.773036em;"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.095em;vertical-align:-0.345em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span> disk accesses, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span> is the size of one page in the flash memory.</p>

                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@inproceedings{disktrie:walcom07,
    Title                    = {{DiskTrie}: An Efficient Data Structure Using Flash Memory for Mobile Devices},
    Author                   = {N. M. M. K. Chowdhury and M. M. Akbar and M. Kaykobad},
    Booktitle                = {WALCOM},
    Year                     = {2007},
    Month                    = {February},
    Pages                    = {76--87},
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
        <section class="year">
            <div class="year-mark-wrapper">
                <span class="year-mark" data-year="2006"></span>
            </div>
            <ul>
                <li data-pub-venue=""
                    data-pub-cat="">
                    <div class="pub-block">
                        <div class="pub-title">
                            A study of the hybrid admission control algorithm for multimedia server
                            
                        </div>
                        
                        <div class="pub-authors">
                            <span class="pub-author pub-author-N.-M. M. K. Chowdhury ">N.&nbspM.&nbspM.&nbspK.&nbspChowdhury</span>
                        </div>
                        
                        <div class="pub-abstract-frame">
                            <div class="pub-abstract">
                                <blockquote>A multimedia server has to serve a large number of clients simultaneously. Considering the real-time requirements of each client and constant data transfer rate of storage devices, it must employ admission control algorithms to control client traffic in order to increase utilization of server resources. Hence, the main goal of an admission control algorithm is to accept enough traffic to efficiently utilize server resources, while not accepting clients whose admission may lead to violations of the service requirements of pre-existing clients. In this thesis we are examining a hybrid admission control algorithm that can handle a larger number of clients simultaneously. We also consider the use of multiple storage devices with separate file systems and thus increase the parallelism during disk access which considerably improves the server performance. The performance of hybrid admission control algorithm is dependent on the disk-scheduling algorithm employed and hence on the time required to retrieve necessary disk blocks to satiate client requests. In this thesis we demonstrate the effect of using different disk scheduling algorithms on the performance of the hybrid admission control algorithm. For each disk scheduling algorithm we consider minimizing both the rotational latency and the seek time. In order to further decrease the service time we ensure parallelism by introducing concurrent disk access through separate file systems for separate disks. To provide continuous retrieval of each media stream, we have to ensure that service time is less than the minimum duration of a round. Since the service time is a function of the number of blocks and their relative positions on the disks, it may exceed the minimum duration of a round. We refer to such rounds as overflow rounds. We also introduce some parameters to restrict overflow rounds within limits. We have devised an extension to the Hybrid Admission Control Algorithm to provide for multiple storage devices and analyzed its complexity. Finally, we have demonstrated the increment in performance and utilization of multimedia server using both normal and extended algorithm with multi-storage and effects of using different disk scheduling algorithms alongside them through extensive simulation.
                                </blockquote>
                            </div>
                        </div>
                        <div class="pub-links">
                            <a class="pub-link-bibtex" data-clipboard-text="@misc{nmmkchow:bscthesis06,
    Title                    = {A Study of the Hybrid Admission Control Algorithm for Multimedia Server},
    Author                   = {N. M. M. K. Chowdhury},
    Month                    = {November},
    Year                     = {2006},
    Abstract                 = {A multimedia server has to serve a large number of clients simultaneously. Considering the real-time requirements of each client and constant data transfer rate of storage devices, it must employ admission control algorithms to control client traffic in order to increase utilization of server resources. Hence, the main goal of an admission control algorithm is to accept enough traffic to efficiently utilize server resources, while not accepting clients whose admission may lead to violations of the service requirements of pre-existing clients. In this thesis we are examining a hybrid admission control algorithm that can handle a larger number of clients simultaneously. We also consider the use of multiple storage devices with separate file systems and thus increase the parallelism during disk access which considerably improves the server performance. 

The performance of hybrid admission control algorithm is dependent on the disk-scheduling algorithm employed and hence on the time required to retrieve necessary disk blocks to satiate client requests. In this thesis we demonstrate the effect of using different disk scheduling algorithms on the performance of the hybrid admission control algorithm. For each disk scheduling algorithm we consider minimizing both the rotational latency and the seek time. In order to further decrease the service time we ensure parallelism by introducing concurrent disk access through separate file systems for separate disks. To provide continuous retrieval of each media stream, we have to ensure that service time is less than the minimum duration of a round. Since the service time is a function of the number of blocks and their relative positions on the disks, it may exceed the minimum duration of a round. We refer to such rounds as overflow rounds. We also introduce some parameters to restrict overflow rounds within limits.

We have devised an extension to the Hybrid Admission Control Algorithm to provide for multiple storage devices and analyzed its complexity. Finally, we have demonstrated the increment in performance and utilization of multimedia server using both normal and extended algorithm with multi-storage and effects of using different disk scheduling algorithms alongside them through extensive simulation.}
}
">[bibtex]</a>
                            <a class="pub-link-abstract">[abstract]</a>
                        </div>
                    </div>
                </li>
            </ul>
        </section>
    </div> <!-- pub-list -->
</div> <!-- publist -->


      </div>
      
      
      
    </div>
    

    
    
    


        </div>
        

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SymbioticLab</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  















  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">


  


</body>
</html>
